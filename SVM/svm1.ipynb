{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d654006e",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bafa799c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjoblib\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstatistics\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder, StandardScaler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import statistics\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3aad2269",
   "metadata": {},
   "source": [
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d245ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tn, fp, fn, tp = each_index(cm)\n",
    "def each_index(metrix):\n",
    "    TN = metrix[0][0]\n",
    "    FP = metrix[0][1]\n",
    "    FN = metrix[1][0]\n",
    "    TP = metrix[1][1]\n",
    "    return TN, FP, FN, TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate sensitivity\n",
    "    sensitivity = TP / (TP + FN) * 100\n",
    "    print(\"Sensitivity: %.2f%%\" % sensitivity)\n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa929a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate specificity\n",
    "    specificity = TN / (TN + FP) * 100\n",
    "    print(\"Specificity: %.2f%%\" % specificity)\n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate accuracy\n",
    "    accuracy = ((TP + TN) / (TP + TN + FP + FN)) *100\n",
    "    print(\"Accuracy: %.2f%%\" % accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate accuracy\n",
    "    precision = (TP / (TP + FP)) *100\n",
    "    print(\"Precision: %.2f%%\" % precision)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate f1\n",
    "    recall = sensitivity(metrix)\n",
    "    pre = precision(metrix)\n",
    "    f1 = ((2*pre*recall) / (pre+recall))\n",
    "    print(\"F1: %.2f%%\" % f1)\n",
    "    return f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a01cd536",
   "metadata": {},
   "source": [
    "parameter1 split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split1'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b78b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search svm model\n",
    "\n",
    "# define model\n",
    "model = SVC()\n",
    "\n",
    "# define parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['linear', 'rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(model, param_grid, refit = True)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(x_val, y_val)\n",
    "\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the individual models\n",
    "model = SVC(C=10, gamma=0.001, kernel='rbf')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param1/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split1 = sensitivity(cm_test)\n",
    "spec_test_split1 = specificity(cm_test)\n",
    "acc_test_split1 = accuracy(cm_test)\n",
    "pre_test_split1 = precision(cm_test)\n",
    "f1_test_split1 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split1 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split1)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split1))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5ec0a",
   "metadata": {},
   "source": [
    "split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca7bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split2'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param1/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be203a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split2 = sensitivity(cm_test)\n",
    "spec_test_split2 = specificity(cm_test)\n",
    "acc_test_split2 = accuracy(cm_test)\n",
    "pre_test_split2 = precision(cm_test)\n",
    "f1_test_split2 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split2 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split2)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split2))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde69e32",
   "metadata": {},
   "source": [
    "split3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split3'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param1/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71342927",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split3 = sensitivity(cm_test)\n",
    "spec_test_split3 = specificity(cm_test)\n",
    "acc_test_split3 = accuracy(cm_test)\n",
    "pre_test_split3 = precision(cm_test)\n",
    "f1_test_split3 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split3 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split3)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split3))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622066b5",
   "metadata": {},
   "source": [
    "split4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b505d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split4'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param1/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split4 = sensitivity(cm_test)\n",
    "spec_test_split4 = specificity(cm_test)\n",
    "acc_test_split4 = accuracy(cm_test)\n",
    "pre_test_split4 = precision(cm_test)\n",
    "f1_test_split4 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split4 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split4)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split4))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618dcdd8",
   "metadata": {},
   "source": [
    "split5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split5'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param1/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split5 = sensitivity(cm_test)\n",
    "spec_test_split5 = specificity(cm_test)\n",
    "acc_test_split5 = accuracy(cm_test)\n",
    "pre_test_split5 = precision(cm_test)\n",
    "f1_test_split5 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split5 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split5)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split5))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb8ed102",
   "metadata": {},
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6afdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_values = []\n",
    "\n",
    "sensitivity_values.append(sen_test_split1)\n",
    "sensitivity_values.append(sen_test_split2)\n",
    "sensitivity_values.append(sen_test_split3)\n",
    "sensitivity_values.append(sen_test_split4)\n",
    "sensitivity_values.append(sen_test_split5)\n",
    "\n",
    "print(sensitivity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_sensitivity = statistics.mean(sensitivity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "sensitivity_sd = statistics.stdev(sensitivity_values)\n",
    "\n",
    "print(\"Mean Sensitivity:\", mean_sensitivity)\n",
    "print(\"Sensitivity Standard Deviation:\", sensitivity_sd)\n",
    "\n",
    "# Print mean sensitivity with standard deviation\n",
    "print(\"Overall Sensitivity: %.2f ± %.2f%%\" % (mean_sensitivity, sensitivity_sd))\n",
    "\n",
    "specificity_values = []\n",
    "\n",
    "specificity_values.append(spec_test_split1)\n",
    "specificity_values.append(spec_test_split2)\n",
    "specificity_values.append(spec_test_split3)\n",
    "specificity_values.append(spec_test_split4)\n",
    "specificity_values.append(spec_test_split5)\n",
    "    \n",
    "print(specificity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_specificity = statistics.mean(specificity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "specificity_sd = statistics.stdev(specificity_values)\n",
    "\n",
    "print(\"Mean Specificity:\", mean_specificity)\n",
    "print(\"Specificity Standard Deviation:\", specificity_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall Specificity: %.2f ± %.2f%%\" % (mean_specificity, specificity_sd))\n",
    "\n",
    "accuracy_values = []\n",
    "\n",
    "accuracy_values.append(acc_test_split1)\n",
    "accuracy_values.append(acc_test_split2)\n",
    "accuracy_values.append(acc_test_split3)\n",
    "accuracy_values.append(acc_test_split4)\n",
    "accuracy_values.append(acc_test_split5)\n",
    "    \n",
    "print(accuracy_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_accuracy = statistics.mean(accuracy_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "accuracy_sd = statistics.stdev(accuracy_values)\n",
    "\n",
    "print(\"Mean accuracy:\", mean_accuracy)\n",
    "print(\"accuracy Standard Deviation:\", accuracy_sd)\n",
    "\n",
    "# Print mean accuracy with standard deviation\n",
    "print(\"Overall accuracy: %.2f ± %.2f%%\" % (mean_accuracy, accuracy_sd))\n",
    "\n",
    "precision_values = []\n",
    "\n",
    "precision_values.append(pre_test_split1)\n",
    "precision_values.append(pre_test_split2)\n",
    "precision_values.append(pre_test_split3)\n",
    "precision_values.append(pre_test_split4)\n",
    "precision_values.append(pre_test_split5)\n",
    "    \n",
    "print(precision_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_precision = statistics.mean(precision_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "precision_sd = statistics.stdev(precision_values)\n",
    "\n",
    "print(\"Mean precision:\", mean_precision)\n",
    "print(\"precision Standard Deviation:\", precision_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall precision: %.2f ± %.2f%%\" % (mean_precision, precision_sd))\n",
    "\n",
    "f1_values = []\n",
    "\n",
    "f1_values.append(f1_test_split1)\n",
    "f1_values.append(f1_test_split2)\n",
    "f1_values.append(f1_test_split3)\n",
    "f1_values.append(f1_test_split4)\n",
    "f1_values.append(f1_test_split5)\n",
    "    \n",
    "print(f1_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_f1 = statistics.mean(f1_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "f1_sd = statistics.stdev(f1_values)\n",
    "\n",
    "print(\"Mean f1:\", mean_f1)\n",
    "print(\"f1 Standard Deviation:\", f1_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall f1: %.2f ± %.2f%%\" % (mean_f1, f1_sd))\n",
    "\n",
    "auc_values = []\n",
    "\n",
    "auc_values.append(auc_test_split1)\n",
    "auc_values.append(auc_test_split2)\n",
    "auc_values.append(auc_test_split3)\n",
    "auc_values.append(auc_test_split4)\n",
    "auc_values.append(auc_test_split5)\n",
    "    \n",
    "print(auc_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_auc = statistics.mean(auc_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "auc_sd = statistics.stdev(auc_values)\n",
    "\n",
    "print(\"Mean auc:\", mean_auc)\n",
    "print(\"auc Standard Deviation:\", auc_sd)\n",
    "\n",
    "# Print mean auc with standard deviation\n",
    "print(\"Overall auc: %.2f ± %.2f%%\" % (mean_auc, auc_sd))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d0c02bb",
   "metadata": {},
   "source": [
    "parameter2 split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ad864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split2'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search svm model\n",
    "\n",
    "# define model\n",
    "model = SVC()\n",
    "\n",
    "# define parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['linear', 'rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(model, param_grid, refit = True)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(x_val, y_val)\n",
    "\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56889fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the individual models\n",
    "model = SVC(C=10, gamma=0.01, kernel='sigmoid')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param2/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca5b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split2 = sensitivity(cm_test)\n",
    "spec_test_split2 = specificity(cm_test)\n",
    "acc_test_split2 = accuracy(cm_test)\n",
    "pre_test_split2 = precision(cm_test)\n",
    "f1_test_split2 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split2 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split2)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split2))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "842ff1ac",
   "metadata": {},
   "source": [
    "split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b35d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split1'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param2/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split1 = sensitivity(cm_test)\n",
    "spec_test_split1 = specificity(cm_test)\n",
    "acc_test_split1 = accuracy(cm_test)\n",
    "pre_test_split1 = precision(cm_test)\n",
    "f1_test_split1 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split1 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split1)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split1))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2dc2f1c",
   "metadata": {},
   "source": [
    "split3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf889e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split3'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param2/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split3 = sensitivity(cm_test)\n",
    "spec_test_split3 = specificity(cm_test)\n",
    "acc_test_split3 = accuracy(cm_test)\n",
    "pre_test_split3 = precision(cm_test)\n",
    "f1_test_split3 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split3 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split3)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split3))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb45d15e",
   "metadata": {},
   "source": [
    "split4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a132de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split4'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param2/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c384450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split4 = sensitivity(cm_test)\n",
    "spec_test_split4 = specificity(cm_test)\n",
    "acc_test_split4 = accuracy(cm_test)\n",
    "pre_test_split4 = precision(cm_test)\n",
    "f1_test_split4 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split4 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split4)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split4))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9197afe5",
   "metadata": {},
   "source": [
    "split5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split5'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param2/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcae51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split5 = sensitivity(cm_test)\n",
    "spec_test_split5 = specificity(cm_test)\n",
    "acc_test_split5 = accuracy(cm_test)\n",
    "pre_test_split5 = precision(cm_test)\n",
    "f1_test_split5 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split5 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split5)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split5))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a30eb283",
   "metadata": {},
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split1 = np.array([[46, 0], [3, 65]])\n",
    "test_split2 = np.array([[46, 0], [6, 62]])\n",
    "test_split3 = np.array([[46, 0], [5, 63]])\n",
    "test_split4 = np.array([[46, 0], [4, 64]])\n",
    "test_split5 = np.array([[46, 0], [3, 65]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37fc5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_test = test_split1 + test_split2 + test_split3 + test_split4 + test_split5\n",
    "print(overall_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d628198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = overall_test.astype('float') / overall_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(overall_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760effad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_values = []\n",
    "\n",
    "sensitivity_values.append(sen_test_split1)\n",
    "sensitivity_values.append(sen_test_split2)\n",
    "sensitivity_values.append(sen_test_split3)\n",
    "sensitivity_values.append(sen_test_split4)\n",
    "sensitivity_values.append(sen_test_split5)\n",
    "\n",
    "print(sensitivity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_sensitivity = statistics.mean(sensitivity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "sensitivity_sd = statistics.stdev(sensitivity_values)\n",
    "\n",
    "print(\"Mean Sensitivity:\", mean_sensitivity)\n",
    "print(\"Sensitivity Standard Deviation:\", sensitivity_sd)\n",
    "\n",
    "# Print mean sensitivity with standard deviation\n",
    "print(\"Overall Sensitivity: %.2f ± %.2f%%\" % (mean_sensitivity, sensitivity_sd))\n",
    "\n",
    "specificity_values = []\n",
    "\n",
    "specificity_values.append(spec_test_split1)\n",
    "specificity_values.append(spec_test_split2)\n",
    "specificity_values.append(spec_test_split3)\n",
    "specificity_values.append(spec_test_split4)\n",
    "specificity_values.append(spec_test_split5)\n",
    "    \n",
    "print(specificity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_specificity = statistics.mean(specificity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "specificity_sd = statistics.stdev(specificity_values)\n",
    "\n",
    "print(\"Mean Specificity:\", mean_specificity)\n",
    "print(\"Specificity Standard Deviation:\", specificity_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall Specificity: %.2f ± %.2f%%\" % (mean_specificity, specificity_sd))\n",
    "\n",
    "accuracy_values = []\n",
    "\n",
    "accuracy_values.append(acc_test_split1)\n",
    "accuracy_values.append(acc_test_split2)\n",
    "accuracy_values.append(acc_test_split3)\n",
    "accuracy_values.append(acc_test_split4)\n",
    "accuracy_values.append(acc_test_split5)\n",
    "    \n",
    "print(accuracy_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_accuracy = statistics.mean(accuracy_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "accuracy_sd = statistics.stdev(accuracy_values)\n",
    "\n",
    "print(\"Mean accuracy:\", mean_accuracy)\n",
    "print(\"accuracy Standard Deviation:\", accuracy_sd)\n",
    "\n",
    "# Print mean accuracy with standard deviation\n",
    "print(\"Overall accuracy: %.2f ± %.2f%%\" % (mean_accuracy, accuracy_sd))\n",
    "\n",
    "precision_values = []\n",
    "\n",
    "precision_values.append(pre_test_split1)\n",
    "precision_values.append(pre_test_split2)\n",
    "precision_values.append(pre_test_split3)\n",
    "precision_values.append(pre_test_split4)\n",
    "precision_values.append(pre_test_split5)\n",
    "    \n",
    "print(precision_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_precision = statistics.mean(precision_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "precision_sd = statistics.stdev(precision_values)\n",
    "\n",
    "print(\"Mean precision:\", mean_precision)\n",
    "print(\"precision Standard Deviation:\", precision_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall precision: %.2f ± %.2f%%\" % (mean_precision, precision_sd))\n",
    "\n",
    "f1_values = []\n",
    "\n",
    "f1_values.append(f1_test_split1)\n",
    "f1_values.append(f1_test_split2)\n",
    "f1_values.append(f1_test_split3)\n",
    "f1_values.append(f1_test_split4)\n",
    "f1_values.append(f1_test_split5)\n",
    "    \n",
    "print(f1_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_f1 = statistics.mean(f1_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "f1_sd = statistics.stdev(f1_values)\n",
    "\n",
    "print(\"Mean f1:\", mean_f1)\n",
    "print(\"f1 Standard Deviation:\", f1_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall f1: %.2f ± %.2f%%\" % (mean_f1, f1_sd))\n",
    "\n",
    "auc_values = []\n",
    "\n",
    "auc_values.append(auc_test_split1)\n",
    "auc_values.append(auc_test_split2)\n",
    "auc_values.append(auc_test_split3)\n",
    "auc_values.append(auc_test_split4)\n",
    "auc_values.append(auc_test_split5)\n",
    "    \n",
    "print(auc_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_auc = statistics.mean(auc_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "auc_sd = statistics.stdev(auc_values)\n",
    "\n",
    "print(\"Mean auc:\", mean_auc)\n",
    "print(\"auc Standard Deviation:\", auc_sd)\n",
    "\n",
    "# Print mean auc with standard deviation\n",
    "print(\"Overall auc: %.2f ± %.2f%%\" % (mean_auc, auc_sd))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2aa58b7",
   "metadata": {},
   "source": [
    "parameter3 split3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split3'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search svm model\n",
    "\n",
    "# define model\n",
    "model = SVC()\n",
    "\n",
    "# define parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['linear', 'rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(model, param_grid, refit = True)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(x_val, y_val)\n",
    "\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the individual models\n",
    "model = SVC(C=0.1, gamma=0.1, kernel='sigmoid')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param3/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d4390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split3 = sensitivity(cm_test)\n",
    "spec_test_split3 = specificity(cm_test)\n",
    "acc_test_split3 = accuracy(cm_test)\n",
    "pre_test_split3 = precision(cm_test)\n",
    "f1_test_split3 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split3 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split3)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split3))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44019e87",
   "metadata": {},
   "source": [
    "split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split1'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param3/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split1 = sensitivity(cm_test)\n",
    "spec_test_split1 = specificity(cm_test)\n",
    "acc_test_split1 = accuracy(cm_test)\n",
    "pre_test_split1 = precision(cm_test)\n",
    "f1_test_split1 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split1 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split1)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split1))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cd0d1ce",
   "metadata": {},
   "source": [
    "split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efed21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split2'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param3/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split2 = sensitivity(cm_test)\n",
    "spec_test_split2 = specificity(cm_test)\n",
    "acc_test_split2 = accuracy(cm_test)\n",
    "pre_test_split2 = precision(cm_test)\n",
    "f1_test_split2 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split2 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split2)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split2))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b42d762",
   "metadata": {},
   "source": [
    "split4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split4'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param3/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5828d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split4 = sensitivity(cm_test)\n",
    "spec_test_split4 = specificity(cm_test)\n",
    "acc_test_split4 = accuracy(cm_test)\n",
    "pre_test_split4 = precision(cm_test)\n",
    "f1_test_split4 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split4 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split4)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split4))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad36ddb6",
   "metadata": {},
   "source": [
    "split5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split5'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param3/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split5 = sensitivity(cm_test)\n",
    "spec_test_split5 = specificity(cm_test)\n",
    "acc_test_split5 = accuracy(cm_test)\n",
    "pre_test_split5 = precision(cm_test)\n",
    "f1_test_split5 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split5 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split5)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split5))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd75ee6d",
   "metadata": {},
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff594fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_values = []\n",
    "\n",
    "sensitivity_values.append(sen_test_split1)\n",
    "sensitivity_values.append(sen_test_split2)\n",
    "sensitivity_values.append(sen_test_split3)\n",
    "sensitivity_values.append(sen_test_split4)\n",
    "sensitivity_values.append(sen_test_split5)\n",
    "\n",
    "print(sensitivity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_sensitivity = statistics.mean(sensitivity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "sensitivity_sd = statistics.stdev(sensitivity_values)\n",
    "\n",
    "print(\"Mean Sensitivity:\", mean_sensitivity)\n",
    "print(\"Sensitivity Standard Deviation:\", sensitivity_sd)\n",
    "\n",
    "# Print mean sensitivity with standard deviation\n",
    "print(\"Overall Sensitivity: %.2f ± %.2f%%\" % (mean_sensitivity, sensitivity_sd))\n",
    "\n",
    "specificity_values = []\n",
    "\n",
    "specificity_values.append(spec_test_split1)\n",
    "specificity_values.append(spec_test_split2)\n",
    "specificity_values.append(spec_test_split3)\n",
    "specificity_values.append(spec_test_split4)\n",
    "specificity_values.append(spec_test_split5)\n",
    "    \n",
    "print(specificity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_specificity = statistics.mean(specificity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "specificity_sd = statistics.stdev(specificity_values)\n",
    "\n",
    "print(\"Mean Specificity:\", mean_specificity)\n",
    "print(\"Specificity Standard Deviation:\", specificity_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall Specificity: %.2f ± %.2f%%\" % (mean_specificity, specificity_sd))\n",
    "\n",
    "accuracy_values = []\n",
    "\n",
    "accuracy_values.append(acc_test_split1)\n",
    "accuracy_values.append(acc_test_split2)\n",
    "accuracy_values.append(acc_test_split3)\n",
    "accuracy_values.append(acc_test_split4)\n",
    "accuracy_values.append(acc_test_split5)\n",
    "    \n",
    "print(accuracy_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_accuracy = statistics.mean(accuracy_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "accuracy_sd = statistics.stdev(accuracy_values)\n",
    "\n",
    "print(\"Mean accuracy:\", mean_accuracy)\n",
    "print(\"accuracy Standard Deviation:\", accuracy_sd)\n",
    "\n",
    "# Print mean accuracy with standard deviation\n",
    "print(\"Overall accuracy: %.2f ± %.2f%%\" % (mean_accuracy, accuracy_sd))\n",
    "\n",
    "precision_values = []\n",
    "\n",
    "precision_values.append(pre_test_split1)\n",
    "precision_values.append(pre_test_split2)\n",
    "precision_values.append(pre_test_split3)\n",
    "precision_values.append(pre_test_split4)\n",
    "precision_values.append(pre_test_split5)\n",
    "    \n",
    "print(precision_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_precision = statistics.mean(precision_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "precision_sd = statistics.stdev(precision_values)\n",
    "\n",
    "print(\"Mean precision:\", mean_precision)\n",
    "print(\"precision Standard Deviation:\", precision_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall precision: %.2f ± %.2f%%\" % (mean_precision, precision_sd))\n",
    "\n",
    "f1_values = []\n",
    "\n",
    "f1_values.append(f1_test_split1)\n",
    "f1_values.append(f1_test_split2)\n",
    "f1_values.append(f1_test_split3)\n",
    "f1_values.append(f1_test_split4)\n",
    "f1_values.append(f1_test_split5)\n",
    "    \n",
    "print(f1_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_f1 = statistics.mean(f1_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "f1_sd = statistics.stdev(f1_values)\n",
    "\n",
    "print(\"Mean f1:\", mean_f1)\n",
    "print(\"f1 Standard Deviation:\", f1_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall f1: %.2f ± %.2f%%\" % (mean_f1, f1_sd))\n",
    "\n",
    "auc_values = []\n",
    "\n",
    "auc_values.append(auc_test_split1)\n",
    "auc_values.append(auc_test_split2)\n",
    "auc_values.append(auc_test_split3)\n",
    "auc_values.append(auc_test_split4)\n",
    "auc_values.append(auc_test_split5)\n",
    "    \n",
    "print(auc_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_auc = statistics.mean(auc_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "auc_sd = statistics.stdev(auc_values)\n",
    "\n",
    "print(\"Mean auc:\", mean_auc)\n",
    "print(\"auc Standard Deviation:\", auc_sd)\n",
    "\n",
    "# Print mean auc with standard deviation\n",
    "print(\"Overall auc: %.2f ± %.2f%%\" % (mean_auc, auc_sd))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d25aa45c",
   "metadata": {},
   "source": [
    "parameter4 split4 (ซ้ำกับ split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d806482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split4'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search svm model\n",
    "\n",
    "# define model\n",
    "model = SVC()\n",
    "\n",
    "# define parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['linear', 'rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(model, param_grid, refit = True)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(x_val, y_val)\n",
    "\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76038019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the individual models\n",
    "model = SVC(C=10, gamma=0.01, kernel='sigmoid')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param4/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split4 = sensitivity(cm_test)\n",
    "spec_test_split4 = specificity(cm_test)\n",
    "acc_test_split4 = accuracy(cm_test)\n",
    "pre_test_split4 = precision(cm_test)\n",
    "f1_test_split4 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split4 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split4)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split4))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf0ff771",
   "metadata": {},
   "source": [
    "split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4008ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split1'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param4/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split1 = sensitivity(cm_test)\n",
    "spec_test_split1 = specificity(cm_test)\n",
    "acc_test_split1 = accuracy(cm_test)\n",
    "pre_test_split1 = precision(cm_test)\n",
    "f1_test_split1 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split1 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split1)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split1))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef63b6ee",
   "metadata": {},
   "source": [
    "split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbd107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split2'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param4/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6239d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split2 = sensitivity(cm_test)\n",
    "spec_test_split2 = specificity(cm_test)\n",
    "acc_test_split2 = accuracy(cm_test)\n",
    "pre_test_split2 = precision(cm_test)\n",
    "f1_test_split2 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split2 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split2)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split2))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fdcaaec",
   "metadata": {},
   "source": [
    "split3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split3'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param4/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split3 = sensitivity(cm_test)\n",
    "spec_test_split3 = specificity(cm_test)\n",
    "acc_test_split3 = accuracy(cm_test)\n",
    "pre_test_split3 = precision(cm_test)\n",
    "f1_test_split3 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split3 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split3)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split3))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07285a29",
   "metadata": {},
   "source": [
    "split5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc90a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split5'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param4/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split5 = sensitivity(cm_test)\n",
    "spec_test_split5 = specificity(cm_test)\n",
    "acc_test_split5 = accuracy(cm_test)\n",
    "pre_test_split5 = precision(cm_test)\n",
    "f1_test_split5 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split5 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split5)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split5))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbf624e3",
   "metadata": {},
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_values = []\n",
    "\n",
    "sensitivity_values.append(sen_test_split1)\n",
    "sensitivity_values.append(sen_test_split2)\n",
    "sensitivity_values.append(sen_test_split3)\n",
    "sensitivity_values.append(sen_test_split4)\n",
    "sensitivity_values.append(sen_test_split5)\n",
    "\n",
    "print(sensitivity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_sensitivity = statistics.mean(sensitivity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "sensitivity_sd = statistics.stdev(sensitivity_values)\n",
    "\n",
    "print(\"Mean Sensitivity:\", mean_sensitivity)\n",
    "print(\"Sensitivity Standard Deviation:\", sensitivity_sd)\n",
    "\n",
    "# Print mean sensitivity with standard deviation\n",
    "print(\"Overall Sensitivity: %.2f ± %.2f%%\" % (mean_sensitivity, sensitivity_sd))\n",
    "\n",
    "specificity_values = []\n",
    "\n",
    "specificity_values.append(spec_test_split1)\n",
    "specificity_values.append(spec_test_split2)\n",
    "specificity_values.append(spec_test_split3)\n",
    "specificity_values.append(spec_test_split4)\n",
    "specificity_values.append(spec_test_split5)\n",
    "    \n",
    "print(specificity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_specificity = statistics.mean(specificity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "specificity_sd = statistics.stdev(specificity_values)\n",
    "\n",
    "print(\"Mean Specificity:\", mean_specificity)\n",
    "print(\"Specificity Standard Deviation:\", specificity_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall Specificity: %.2f ± %.2f%%\" % (mean_specificity, specificity_sd))\n",
    "\n",
    "accuracy_values = []\n",
    "\n",
    "accuracy_values.append(acc_test_split1)\n",
    "accuracy_values.append(acc_test_split2)\n",
    "accuracy_values.append(acc_test_split3)\n",
    "accuracy_values.append(acc_test_split4)\n",
    "accuracy_values.append(acc_test_split5)\n",
    "    \n",
    "print(accuracy_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_accuracy = statistics.mean(accuracy_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "accuracy_sd = statistics.stdev(accuracy_values)\n",
    "\n",
    "print(\"Mean accuracy:\", mean_accuracy)\n",
    "print(\"accuracy Standard Deviation:\", accuracy_sd)\n",
    "\n",
    "# Print mean accuracy with standard deviation\n",
    "print(\"Overall accuracy: %.2f ± %.2f%%\" % (mean_accuracy, accuracy_sd))\n",
    "\n",
    "precision_values = []\n",
    "\n",
    "precision_values.append(pre_test_split1)\n",
    "precision_values.append(pre_test_split2)\n",
    "precision_values.append(pre_test_split3)\n",
    "precision_values.append(pre_test_split4)\n",
    "precision_values.append(pre_test_split5)\n",
    "    \n",
    "print(precision_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_precision = statistics.mean(precision_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "precision_sd = statistics.stdev(precision_values)\n",
    "\n",
    "print(\"Mean precision:\", mean_precision)\n",
    "print(\"precision Standard Deviation:\", precision_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall precision: %.2f ± %.2f%%\" % (mean_precision, precision_sd))\n",
    "\n",
    "f1_values = []\n",
    "\n",
    "f1_values.append(f1_test_split1)\n",
    "f1_values.append(f1_test_split2)\n",
    "f1_values.append(f1_test_split3)\n",
    "f1_values.append(f1_test_split4)\n",
    "f1_values.append(f1_test_split5)\n",
    "    \n",
    "print(f1_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_f1 = statistics.mean(f1_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "f1_sd = statistics.stdev(f1_values)\n",
    "\n",
    "print(\"Mean f1:\", mean_f1)\n",
    "print(\"f1 Standard Deviation:\", f1_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall f1: %.2f ± %.2f%%\" % (mean_f1, f1_sd))\n",
    "\n",
    "auc_values = []\n",
    "\n",
    "auc_values.append(auc_test_split1)\n",
    "auc_values.append(auc_test_split2)\n",
    "auc_values.append(auc_test_split3)\n",
    "auc_values.append(auc_test_split4)\n",
    "auc_values.append(auc_test_split5)\n",
    "    \n",
    "print(auc_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_auc = statistics.mean(auc_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "auc_sd = statistics.stdev(auc_values)\n",
    "\n",
    "print(\"Mean auc:\", mean_auc)\n",
    "print(\"auc Standard Deviation:\", auc_sd)\n",
    "\n",
    "# Print mean auc with standard deviation\n",
    "print(\"Overall auc: %.2f ± %.2f%%\" % (mean_auc, auc_sd))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04738889",
   "metadata": {},
   "source": [
    "parameter5 split5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f73b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split5'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search svm model\n",
    "\n",
    "# define model\n",
    "model = SVC()\n",
    "\n",
    "# define parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['linear', 'rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(model, param_grid, refit = True)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(x_val, y_val)\n",
    "\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab39974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the individual models\n",
    "model = SVC(C=0.1, gamma=1, kernel='linear')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param5/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a29e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split5 = sensitivity(cm_test)\n",
    "spec_test_split5 = specificity(cm_test)\n",
    "acc_test_split5 = accuracy(cm_test)\n",
    "pre_test_split5 = precision(cm_test)\n",
    "f1_test_split5 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split5 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split5)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split5))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f70fcdbf",
   "metadata": {},
   "source": [
    "split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a41767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split1'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param5/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split1 = sensitivity(cm_test)\n",
    "spec_test_split1 = specificity(cm_test)\n",
    "acc_test_split1 = accuracy(cm_test)\n",
    "pre_test_split1 = precision(cm_test)\n",
    "f1_test_split1 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split1 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split1)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split1))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9cd87c5",
   "metadata": {},
   "source": [
    "split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ba437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split2'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param5/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split2 = sensitivity(cm_test)\n",
    "spec_test_split2 = specificity(cm_test)\n",
    "acc_test_split2 = accuracy(cm_test)\n",
    "pre_test_split2 = precision(cm_test)\n",
    "f1_test_split2 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split2 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split2)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split2))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3313d88f",
   "metadata": {},
   "source": [
    "split3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split3'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param5/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e085dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split3 = sensitivity(cm_test)\n",
    "spec_test_split3 = specificity(cm_test)\n",
    "acc_test_split3 = accuracy(cm_test)\n",
    "pre_test_split3 = precision(cm_test)\n",
    "f1_test_split3 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split3 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split3)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split3))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f38ca11",
   "metadata": {},
   "source": [
    "split4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fdfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split4'\n",
    "path = \"../Anny_oversampling/DATA\"\n",
    "train_path = \"{}/{}/train/oversampled_train_{}.csv\".format(path,split,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val_{}.csv\".format(path,split,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "x_val = sc.fit_transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../SVM/models/param5/svm_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split4 = sensitivity(cm_test)\n",
    "spec_test_split4 = specificity(cm_test)\n",
    "acc_test_split4 = accuracy(cm_test)\n",
    "pre_test_split4 = precision(cm_test)\n",
    "f1_test_split4 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split4 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split4)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split4))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a0969b7",
   "metadata": {},
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ac61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_values = []\n",
    "\n",
    "sensitivity_values.append(sen_test_split1)\n",
    "sensitivity_values.append(sen_test_split2)\n",
    "sensitivity_values.append(sen_test_split3)\n",
    "sensitivity_values.append(sen_test_split4)\n",
    "sensitivity_values.append(sen_test_split5)\n",
    "\n",
    "print(sensitivity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_sensitivity = statistics.mean(sensitivity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "sensitivity_sd = statistics.stdev(sensitivity_values)\n",
    "\n",
    "print(\"Mean Sensitivity:\", mean_sensitivity)\n",
    "print(\"Sensitivity Standard Deviation:\", sensitivity_sd)\n",
    "\n",
    "# Print mean sensitivity with standard deviation\n",
    "print(\"Overall Sensitivity: %.2f ± %.2f%%\" % (mean_sensitivity, sensitivity_sd))\n",
    "\n",
    "specificity_values = []\n",
    "\n",
    "specificity_values.append(spec_test_split1)\n",
    "specificity_values.append(spec_test_split2)\n",
    "specificity_values.append(spec_test_split3)\n",
    "specificity_values.append(spec_test_split4)\n",
    "specificity_values.append(spec_test_split5)\n",
    "    \n",
    "print(specificity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_specificity = statistics.mean(specificity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "specificity_sd = statistics.stdev(specificity_values)\n",
    "\n",
    "print(\"Mean Specificity:\", mean_specificity)\n",
    "print(\"Specificity Standard Deviation:\", specificity_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall Specificity: %.2f ± %.2f%%\" % (mean_specificity, specificity_sd))\n",
    "\n",
    "accuracy_values = []\n",
    "\n",
    "accuracy_values.append(acc_test_split1)\n",
    "accuracy_values.append(acc_test_split2)\n",
    "accuracy_values.append(acc_test_split3)\n",
    "accuracy_values.append(acc_test_split4)\n",
    "accuracy_values.append(acc_test_split5)\n",
    "    \n",
    "print(accuracy_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_accuracy = statistics.mean(accuracy_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "accuracy_sd = statistics.stdev(accuracy_values)\n",
    "\n",
    "print(\"Mean accuracy:\", mean_accuracy)\n",
    "print(\"accuracy Standard Deviation:\", accuracy_sd)\n",
    "\n",
    "# Print mean accuracy with standard deviation\n",
    "print(\"Overall accuracy: %.2f ± %.2f%%\" % (mean_accuracy, accuracy_sd))\n",
    "\n",
    "precision_values = []\n",
    "\n",
    "precision_values.append(pre_test_split1)\n",
    "precision_values.append(pre_test_split2)\n",
    "precision_values.append(pre_test_split3)\n",
    "precision_values.append(pre_test_split4)\n",
    "precision_values.append(pre_test_split5)\n",
    "    \n",
    "print(precision_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_precision = statistics.mean(precision_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "precision_sd = statistics.stdev(precision_values)\n",
    "\n",
    "print(\"Mean precision:\", mean_precision)\n",
    "print(\"precision Standard Deviation:\", precision_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall precision: %.2f ± %.2f%%\" % (mean_precision, precision_sd))\n",
    "\n",
    "f1_values = []\n",
    "\n",
    "f1_values.append(f1_test_split1)\n",
    "f1_values.append(f1_test_split2)\n",
    "f1_values.append(f1_test_split3)\n",
    "f1_values.append(f1_test_split4)\n",
    "f1_values.append(f1_test_split5)\n",
    "    \n",
    "print(f1_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_f1 = statistics.mean(f1_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "f1_sd = statistics.stdev(f1_values)\n",
    "\n",
    "print(\"Mean f1:\", mean_f1)\n",
    "print(\"f1 Standard Deviation:\", f1_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall f1: %.2f ± %.2f%%\" % (mean_f1, f1_sd))\n",
    "\n",
    "auc_values = []\n",
    "\n",
    "auc_values.append(auc_test_split1)\n",
    "auc_values.append(auc_test_split2)\n",
    "auc_values.append(auc_test_split3)\n",
    "auc_values.append(auc_test_split4)\n",
    "auc_values.append(auc_test_split5)\n",
    "    \n",
    "print(auc_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_auc = statistics.mean(auc_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "auc_sd = statistics.stdev(auc_values)\n",
    "\n",
    "print(\"Mean auc:\", mean_auc)\n",
    "print(\"auc Standard Deviation:\", auc_sd)\n",
    "\n",
    "# Print mean auc with standard deviation\n",
    "print(\"Overall auc: %.2f ± %.2f%%\" % (mean_auc, auc_sd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "23bf0724a01b6ea9814e66f76182ea78c0ee849a72ca257c0e116bf83bb4960a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

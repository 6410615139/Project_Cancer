{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d654006e",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64a62bd3-3688-4c05-8bbc-449730623b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bafa799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import statistics\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad2269",
   "metadata": {},
   "source": [
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d245ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tn, fp, fn, tp = each_index(cm)\n",
    "def each_index(metrix):\n",
    "    TN = metrix[0][0]\n",
    "    FP = metrix[0][1]\n",
    "    FN = metrix[1][0]\n",
    "    TP = metrix[1][1]\n",
    "    return TN, FP, FN, TP\n",
    "def sensitivity(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate sensitivity\n",
    "    sensitivity = TP / (TP + FN) * 100\n",
    "    print(\"Sensitivity: %.2f%%\" % sensitivity)\n",
    "    return sensitivity\n",
    "def specificity(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate specificity\n",
    "    specificity = TN / (TN + FP) * 100\n",
    "    print(\"Specificity: %.2f%%\" % specificity)\n",
    "    return specificity\n",
    "def accuracy(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate accuracy\n",
    "    accuracy = ((TP + TN) / (TP + TN + FP + FN)) *100\n",
    "    print(\"Accuracy: %.2f%%\" % accuracy)\n",
    "    return accuracy\n",
    "def precision(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate accuracy\n",
    "    precision = (TP / (TP + FP)) *100\n",
    "    print(\"Precision: %.2f%%\" % precision)\n",
    "    return precision\n",
    "def f1(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate f1\n",
    "    recall = sensitivity(metrix)\n",
    "    pre = precision(metrix)\n",
    "    f1 = ((2*pre*recall) / (pre+recall))\n",
    "    print(\"F1: %.2f%%\" % f1)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "352eb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(split):\n",
    "    # setting split and path\n",
    "    path = \"../kitt/DATA_progress3/\"\n",
    "    train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "    test_path =  \"{}/test.csv\".format(path)\n",
    "    valid_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "    print(train_path)\n",
    "    # Load the train set\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    print(train_data.shape)\n",
    "    x_train = train_data[train_data.columns[2:]]\n",
    "    print(x_train.shape)\n",
    "    y_train = train_data[train_data.columns[1]]\n",
    "    le = LabelEncoder()\n",
    "    y_train = np.array(le.fit_transform(y_train))\n",
    "    print(le.classes_)\n",
    "    print(test_path)\n",
    "    # Load the test set\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    print(test_data.shape)\n",
    "    x_test = test_data[test_data.columns[2:]]\n",
    "    print(x_test.shape)\n",
    "    y_test = test_data[test_data.columns[1]]\n",
    "    le = LabelEncoder()\n",
    "    y_test = np.array(le.fit_transform(y_test))\n",
    "    print(le.classes_)\n",
    "\n",
    "    print(valid_path)\n",
    "    # Load the validation set\n",
    "    val_data = pd.read_csv(valid_path)\n",
    "    print(val_data.shape)\n",
    "    x_val = val_data[val_data.columns[2:]]\n",
    "    print(x_val.shape)\n",
    "    y_val = val_data[val_data.columns[1]]\n",
    "    le = LabelEncoder()\n",
    "    y_val = np.array(le.fit_transform(y_val))\n",
    "    print(le.classes_)\n",
    "\n",
    "    # StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    x_train = x_train.to_numpy()\n",
    "    x_test = x_test.to_numpy()\n",
    "    x_val = x_val.to_numpy()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.transform(x_test)\n",
    "    x_val = sc.transform(x_val)\n",
    "    \n",
    "    return x_train,y_train,x_test,y_test,x_val,y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01cd536",
   "metadata": {},
   "source": [
    "parameter1 split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "55930adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../kitt/DATA_progress3//split1/train/train_oversampling.csv\n",
      "(456, 18)\n",
      "(456, 16)\n",
      "['B' 'M']\n",
      "../kitt/DATA_progress3//test.csv\n",
      "(114, 18)\n",
      "(114, 16)\n",
      "['B' 'M']\n",
      "../kitt/DATA_progress3//split1/val/val.csv\n",
      "(91, 18)\n",
      "(91, 16)\n",
      "['B' 'M']\n"
     ]
    }
   ],
   "source": [
    "split_num = 1\n",
    "split = \"split{}\".format(split_num)\n",
    "x_train,y_train,x_test,y_test,x_val,y_val = setup(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77269bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score_all(cm_val):\n",
    "    sen_val = sensitivity(cm_val)\n",
    "    spec_val = specificity(cm_val)\n",
    "    acc_val = accuracy(cm_val)\n",
    "    pre_val = precision(cm_val)\n",
    "    f1_score_val = f1(cm_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "416bcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(cm_val, save=False,which=\"-\"):\n",
    "    class_names = ['benign','malignant']\n",
    "    # Normalize confusion matrix to percentage\n",
    "    cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "    ax.grid(False)\n",
    "    # Add labels\n",
    "    ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "           yticks=np.arange(cm_norm_val.shape[0]),\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "    ax.set_title(which, fontsize=16)\n",
    "    ax.set_ylabel('Actual', fontsize=16)\n",
    "    ax.set_xlabel('Predicted', fontsize=16)\n",
    "    # Add percentage and count values inside plot\n",
    "    thresh = cm_norm_val.max() / 2.\n",
    "    for i in range(cm_norm_val.shape[0]):\n",
    "        for j in range(cm_norm_val.shape[1]):\n",
    "            ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm_norm_val[i, j] > thresh else \"black\",\n",
    "                   fontsize=20)\n",
    "    if save is not False:\n",
    "        save_path = '{}_{}.png'.format(split)\n",
    "        plt.savefig(save_path)\n",
    "        print(\"Save fig at {}\".format(save_path))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0fda80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC():\n",
    "    fpr1, tpr1, thr1 = metrics.roc_curve(y_val,  y_pred_val)\n",
    "    auc1 = metrics.roc_auc_score(y_val,  y_pred_val)\n",
    "    fpr2, tpr2, thr2 = metrics.roc_curve(y_test,  y_pred_test)\n",
    "    auc2 = metrics.roc_auc_score(y_test,  y_pred_test)\n",
    "    gmeans2 = np.sqrt(tpr2 * (1-fpr2))\n",
    "    ix2 = np.argmax(gmeans2)\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"AUC validate: %.4f\" % auc1)\n",
    "    print(\"AUC test: %.4f\" % auc2)\n",
    "    plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc1))\n",
    "    plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc2))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f245ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    "    \n",
    "def thresholding(y_val, y_pred_val):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_val,  y_pred_val[:, 1])\n",
    "    # get the best threshold\n",
    "    J = tpr - fpr\n",
    "    ix = np.argmax(J)\n",
    "    best_thresh = thresholds[ix]\n",
    "    print('Best Threshold=%f' % (best_thresh))\n",
    "    print('FPR: %.4f\\nTPR: %.4f' %(fpr[ix], tpr[ix]))\n",
    "    y_pred_val_new = to_labels(y_pred_val[:, 1], best_thresh)\n",
    "    return y_pred_val_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1cdf345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(param,split):\n",
    "    #logisticregression/models/param1/lr_model_split1.pkl\n",
    "    model_path = \"../logisticregression/models/param{}/lr_model_split{}.pkl\".format(param,split)\n",
    "    print(model_path)\n",
    "    loaded_model = joblib.load(model_path)\n",
    "\n",
    "    print(\"############## validate set ################\")\n",
    "    y_pred_val_raw = loaded_model.predict_proba(x_val)\n",
    "    y_pred_val = thresholding(y_val, y_pred_val_raw)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "    print('Confusion Matrix')\n",
    "    print('-'*16)\n",
    "    print(cm_val,'\\n')\n",
    "    print('-'*16)\n",
    "\n",
    "    # Calculate score\n",
    "    print(split)\n",
    "    cal_score_all(cm_val)\n",
    "    \n",
    "    # plot confusion matrix\n",
    "    plot_cm(cm_val, which=\"Validate set\")\n",
    "    \n",
    "    # ################# test set ##################\n",
    "    label = ['benign','malignant']\n",
    "    print(\"################# test set ##################\")\n",
    "    y_pred_test_raw = loaded_model.predict_proba(x_test)\n",
    "    y_pred_test = thresholding(y_test, y_pred_test_raw)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "    print('Confusion Matrix')\n",
    "    print('-'*16)\n",
    "    print(cm_test,'\\n')\n",
    "    print('-'*16)\n",
    "\n",
    "    # Calculate score\n",
    "    print(split)\n",
    "    cal_score_all(cm_test)\n",
    "\n",
    "    # plot confusion matrix\n",
    "    plot_cm(cm_test,which=\"Test set\")\n",
    "\n",
    "    ################## plot ROC curve ########################\n",
    "    fpr1, tpr1, thr1 = metrics.roc_curve(y_val,  y_pred_val_raw[:,1])\n",
    "    auc1 = metrics.roc_auc_score(y_val,  y_pred_val_raw[:,1])*100\n",
    "    fpr2, tpr2, thr2 = metrics.roc_curve(y_test,  y_pred_test_raw[:,1])\n",
    "    auc2 = metrics.roc_auc_score(y_test,  y_pred_test_raw[:,1])*100\n",
    "    gmeans2 = np.sqrt(tpr2 * (1-fpr2))\n",
    "    ix2 = np.argmax(gmeans2)\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"AUC validate: %.2f%%\" % auc1)\n",
    "    print(\"AUC test: %.2f%%\" % auc2)\n",
    "    plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.2f%%\" % auc1))\n",
    "    plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.2f%%\" % auc2))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    \n",
    "    return cm_val,cm_test,auc1,auc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "753f273d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../logisticregression/models/param1/lr_model_split1.pkl\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m split \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      2\u001b[0m param \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> 3\u001b[0m param1_split1_cm_val, param1_split1_cm_test, param1_split1_AUC_val, param1_split1_AUC_test \u001b[39m=\u001b[39m evaluate(param,split)\n",
      "Cell \u001b[1;32mIn[65], line 5\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(param, split)\u001b[0m\n\u001b[0;32m      3\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../logisticregression/models/param\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/lr_model_split\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(param,split)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(model_path)\n\u001b[1;32m----> 5\u001b[0m loaded_model \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(model_path)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m############## validate set ################\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m y_pred_val_raw \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39mpredict_proba(x_val)\n",
      "File \u001b[1;32mc:\\Users\\warathep\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m                 \u001b[39mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[1;32m--> 587\u001b[0m             obj \u001b[39m=\u001b[39m _unpickle(fobj, filename, mmap_mode)\n\u001b[0;32m    588\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\warathep\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    504\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 506\u001b[0m     obj \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    507\u001b[0m     \u001b[39mif\u001b[39;00m unpickler\u001b[39m.\u001b[39mcompat_mode:\n\u001b[0;32m    508\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been generated with a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mjoblib version less than 0.10. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mPlease regenerate this pickle file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m                       \u001b[39m%\u001b[39m filename,\n\u001b[0;32m    512\u001b[0m                       \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\warathep\\anaconda3\\lib\\pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1213\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39m)\n\u001b[0;32m   1214\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[0;32m   1215\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "split = 1\n",
    "param = 1\n",
    "param1_split1_cm_val, param1_split1_cm_test, param1_split1_AUC_val, param1_split1_AUC_test = evaluate(param,split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split1'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b78b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search logistic regression model\n",
    "\n",
    "# define model\n",
    "#model = LogisticRegression()\n",
    "\n",
    "# define parameter range\n",
    "#param_grid = {'solver' : ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "#              'penalty' : ['l2'],\n",
    "#              'C' : np.logspace(-4, 4, 50),\n",
    "#              'max_iter' : [1000, 2000, 3000, 4000, 5000]}\n",
    "\n",
    "# define search\n",
    "#search = GridSearchCV(model, param_grid, scoring='accuracy', n_jobs=-1, error_score='raise', cv=5)\n",
    "\n",
    "# execute search\n",
    "#result = search.fit(x_train, y_train)\n",
    "\n",
    "# summarize result\n",
    "#print('Best Score: %s' % result.best_score_)\n",
    "#print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the individual models\n",
    "model = LogisticRegression(C=109.85411419875572, max_iter=1000, penalty='l2', solver='lbfgs')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param1/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807098c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split1 = sensitivity(cm_val)\n",
    "spec_val_split1 = specificity(cm_val)\n",
    "acc_val_split1 = accuracy(cm_val)\n",
    "pre_val_split1 = precision(cm_val)\n",
    "f1_val_split1 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split1 = sensitivity(cm_test)\n",
    "spec_test_split1 = specificity(cm_test)\n",
    "acc_test_split1 = accuracy(cm_test)\n",
    "pre_test_split1 = precision(cm_test)\n",
    "f1_test_split1 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split1 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split1 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split1)\n",
    "print(\"AUC test: %.4f\" % auc_test_split1)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split1))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split1))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5ec0a",
   "metadata": {},
   "source": [
    "split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca7bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split2'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param1/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be203a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split2 = sensitivity(cm_val)\n",
    "spec_val_split2 = specificity(cm_val)\n",
    "acc_val_split2 = accuracy(cm_val)\n",
    "pre_val_split2 = precision(cm_val)\n",
    "f1_val_split2 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split2 = sensitivity(cm_test)\n",
    "spec_test_split2 = specificity(cm_test)\n",
    "acc_test_split2 = accuracy(cm_test)\n",
    "pre_test_split2 = precision(cm_test)\n",
    "f1_test_split2 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split2 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split2 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split2)\n",
    "print(\"AUC test: %.4f\" % auc_test_split2)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split2))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split2))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde69e32",
   "metadata": {},
   "source": [
    "split3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split3'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param1/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71342927",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split3 = sensitivity(cm_val)\n",
    "spec_val_split3 = specificity(cm_val)\n",
    "acc_val_split3 = accuracy(cm_val)\n",
    "pre_val_split3 = precision(cm_val)\n",
    "f1_val_split3 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split3 = sensitivity(cm_test)\n",
    "spec_test_split3 = specificity(cm_test)\n",
    "acc_test_split3 = accuracy(cm_test)\n",
    "pre_test_split3 = precision(cm_test)\n",
    "f1_test_split3 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split3 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split3 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split3)\n",
    "print(\"AUC test: %.4f\" % auc_test_split3)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split3))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split3))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622066b5",
   "metadata": {},
   "source": [
    "split4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b505d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split4'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param1/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split4 = sensitivity(cm_val)\n",
    "spec_val_split4 = specificity(cm_val)\n",
    "acc_val_split4 = accuracy(cm_val)\n",
    "pre_val_split4 = precision(cm_val)\n",
    "f1_val_split4 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split4 = sensitivity(cm_test)\n",
    "spec_test_split4 = specificity(cm_test)\n",
    "acc_test_split4 = accuracy(cm_test)\n",
    "pre_test_split4 = precision(cm_test)\n",
    "f1_test_split4 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split4 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split4 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split4)\n",
    "print(\"AUC test: %.4f\" % auc_test_split4)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split4))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split4))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618dcdd8",
   "metadata": {},
   "source": [
    "split5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split5'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param1/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split5 = sensitivity(cm_val)\n",
    "spec_val_split5 = specificity(cm_val)\n",
    "acc_val_split5 = accuracy(cm_val)\n",
    "pre_val_split5 = precision(cm_val)\n",
    "f1_val_split5 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split5 = sensitivity(cm_test)\n",
    "spec_test_split5 = specificity(cm_test)\n",
    "acc_test_split5 = accuracy(cm_test)\n",
    "pre_test_split5 = precision(cm_test)\n",
    "f1_test_split5 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split5 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split5 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split5)\n",
    "print(\"AUC test: %.4f\" % auc_test_split5)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split5))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split5))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ed102",
   "metadata": {},
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ce982",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_values = []\n",
    "\n",
    "sensitivity_values.append(sen_val_split1)\n",
    "sensitivity_values.append(sen_val_split2)\n",
    "sensitivity_values.append(sen_val_split3)\n",
    "sensitivity_values.append(sen_val_split4)\n",
    "sensitivity_values.append(sen_val_split5)\n",
    "\n",
    "print(sensitivity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_sensitivity = statistics.mean(sensitivity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "sensitivity_sd = statistics.stdev(sensitivity_values)\n",
    "\n",
    "print(\"Mean Sensitivity:\", mean_sensitivity)\n",
    "print(\"Sensitivity Standard Deviation:\", sensitivity_sd)\n",
    "\n",
    "# Print mean sensitivity with standard deviation\n",
    "print(\"Overall Sensitivity: %.2f  %.2f%%\" % (mean_sensitivity, sensitivity_sd))\n",
    "\n",
    "specificity_values = []\n",
    "\n",
    "specificity_values.append(spec_val_split1)\n",
    "specificity_values.append(spec_val_split2)\n",
    "specificity_values.append(spec_val_split3)\n",
    "specificity_values.append(spec_val_split4)\n",
    "specificity_values.append(spec_val_split5)\n",
    "    \n",
    "print(specificity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_specificity = statistics.mean(specificity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "specificity_sd = statistics.stdev(specificity_values)\n",
    "\n",
    "print(\"Mean Specificity:\", mean_specificity)\n",
    "print(\"Specificity Standard Deviation:\", specificity_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall Specificity: %.2f  %.2f%%\" % (mean_specificity, specificity_sd))\n",
    "\n",
    "accuracy_values = []\n",
    "\n",
    "accuracy_values.append(acc_val_split1)\n",
    "accuracy_values.append(acc_val_split2)\n",
    "accuracy_values.append(acc_val_split3)\n",
    "accuracy_values.append(acc_val_split4)\n",
    "accuracy_values.append(acc_val_split5)\n",
    "    \n",
    "print(accuracy_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_accuracy = statistics.mean(accuracy_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "accuracy_sd = statistics.stdev(accuracy_values)\n",
    "\n",
    "print(\"Mean accuracy:\", mean_accuracy)\n",
    "print(\"accuracy Standard Deviation:\", accuracy_sd)\n",
    "\n",
    "# Print mean accuracy with standard deviation\n",
    "print(\"Overall accuracy: %.2f  %.2f%%\" % (mean_accuracy, accuracy_sd))\n",
    "\n",
    "precision_values = []\n",
    "\n",
    "precision_values.append(pre_val_split1)\n",
    "precision_values.append(pre_val_split2)\n",
    "precision_values.append(pre_val_split3)\n",
    "precision_values.append(pre_val_split4)\n",
    "precision_values.append(pre_val_split5)\n",
    "    \n",
    "print(precision_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_precision = statistics.mean(precision_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "precision_sd = statistics.stdev(precision_values)\n",
    "\n",
    "print(\"Mean precision:\", mean_precision)\n",
    "print(\"precision Standard Deviation:\", precision_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall precision: %.2f  %.2f%%\" % (mean_precision, precision_sd))\n",
    "\n",
    "f1_values = []\n",
    "\n",
    "f1_values.append(f1_val_split1)\n",
    "f1_values.append(f1_val_split2)\n",
    "f1_values.append(f1_val_split3)\n",
    "f1_values.append(f1_val_split4)\n",
    "f1_values.append(f1_val_split5)\n",
    "    \n",
    "print(f1_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_f1 = statistics.mean(f1_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "f1_sd = statistics.stdev(f1_values)\n",
    "\n",
    "print(\"Mean f1:\", mean_f1)\n",
    "print(\"f1 Standard Deviation:\", f1_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall f1: %.2f  %.2f%%\" % (mean_f1, f1_sd))\n",
    "\n",
    "auc_values = []\n",
    "\n",
    "auc_values.append(auc_val_split1)\n",
    "auc_values.append(auc_val_split2)\n",
    "auc_values.append(auc_val_split3)\n",
    "auc_values.append(auc_val_split4)\n",
    "auc_values.append(auc_val_split5)\n",
    "    \n",
    "print(auc_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_auc = statistics.mean(auc_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "auc_sd = statistics.stdev(auc_values)\n",
    "\n",
    "print(\"Mean auc:\", mean_auc)\n",
    "print(\"auc Standard Deviation:\", auc_sd)\n",
    "\n",
    "# Print mean auc with standard deviation\n",
    "print(\"Overall auc: %.2f  %.2f%%\" % (mean_auc * 100, auc_sd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c02bb",
   "metadata": {},
   "source": [
    "parameter2 split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ad864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split2'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search logistic regression model\n",
    "\n",
    "# define model\n",
    "#model = LogisticRegression()\n",
    "\n",
    "# define parameter range\n",
    "#param_grid = {'solver' : ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "#              'penalty' : ['l2'],\n",
    "#              'C' : np.logspace(-4, 4, 50),\n",
    "#              'max_iter' : [1000, 2000, 3000, 4000, 5000]}\n",
    "\n",
    "# define search\n",
    "#search = GridSearchCV(model, param_grid, scoring='accuracy', n_jobs=-1, error_score='raise', cv=5)\n",
    "\n",
    "# execute search\n",
    "#result = search.fit(x_train, y_train)\n",
    "\n",
    "# summarize result\n",
    "#print('Best Score: %s' % result.best_score_)\n",
    "#print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56889fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the individual models\n",
    "model = LogisticRegression(C=494.1713361323828, max_iter=1000, penalty='l2', solver='lbfgs')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param2/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca5b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split2 = sensitivity(cm_val)\n",
    "spec_val_split2 = specificity(cm_val)\n",
    "acc_val_split2 = accuracy(cm_val)\n",
    "pre_val_split2 = precision(cm_val)\n",
    "f1_val_split2 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split2 = sensitivity(cm_test)\n",
    "spec_test_split2 = specificity(cm_test)\n",
    "acc_test_split2 = accuracy(cm_test)\n",
    "pre_test_split2 = precision(cm_test)\n",
    "f1_test_split2 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split2 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split2 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split2)\n",
    "print(\"AUC test: %.4f\" % auc_test_split2)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split2))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split2))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ff1ac",
   "metadata": {},
   "source": [
    "split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b35d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split1'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param2/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split1 = sensitivity(cm_val)\n",
    "spec_val_split1 = specificity(cm_val)\n",
    "acc_val_split1 = accuracy(cm_val)\n",
    "pre_val_split1 = precision(cm_val)\n",
    "f1_val_split1 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split1 = sensitivity(cm_test)\n",
    "spec_test_split1 = specificity(cm_test)\n",
    "acc_test_split1 = accuracy(cm_test)\n",
    "pre_test_split1 = precision(cm_test)\n",
    "f1_test_split1 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split1 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split1 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split1)\n",
    "print(\"AUC test: %.4f\" % auc_test_split1)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split1))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split1))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc2f1c",
   "metadata": {},
   "source": [
    "split3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf889e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split3'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param2/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b5bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split3 = sensitivity(cm_val)\n",
    "spec_val_split3 = specificity(cm_val)\n",
    "acc_val_split3 = accuracy(cm_val)\n",
    "pre_val_split3 = precision(cm_val)\n",
    "f1_val_split3 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split3 = sensitivity(cm_test)\n",
    "spec_test_split3 = specificity(cm_test)\n",
    "acc_test_split3 = accuracy(cm_test)\n",
    "pre_test_split3 = precision(cm_test)\n",
    "f1_test_split3 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split3 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split3 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split3)\n",
    "print(\"AUC test: %.4f\" % auc_test_split3)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split3))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split3))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45d15e",
   "metadata": {},
   "source": [
    "split4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a132de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split4'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param2/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c384450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split4 = sensitivity(cm_val)\n",
    "spec_val_split4 = specificity(cm_val)\n",
    "acc_val_split4 = accuracy(cm_val)\n",
    "pre_val_split4 = precision(cm_val)\n",
    "f1_val_split4 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split4 = sensitivity(cm_test)\n",
    "spec_test_split4 = specificity(cm_test)\n",
    "acc_test_split4 = accuracy(cm_test)\n",
    "pre_test_split4 = precision(cm_test)\n",
    "f1_test_split4 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split4 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split4 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split4)\n",
    "print(\"AUC test: %.4f\" % auc_test_split4)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split4))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split4))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197afe5",
   "metadata": {},
   "source": [
    "split5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split5'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param2/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcae51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split5 = sensitivity(cm_val)\n",
    "spec_val_split5 = specificity(cm_val)\n",
    "acc_val_split5 = accuracy(cm_val)\n",
    "pre_val_split5 = precision(cm_val)\n",
    "f1_val_split5 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split5 = sensitivity(cm_test)\n",
    "spec_test_split5 = specificity(cm_test)\n",
    "acc_test_split5 = accuracy(cm_test)\n",
    "pre_test_split5 = precision(cm_test)\n",
    "f1_test_split5 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split5 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split5 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split5)\n",
    "print(\"AUC test: %.4f\" % auc_test_split5)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split5))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split5))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(param,split):\n",
    "    loaded_model = joblib.load('lr_model_final/lr_param{}_{}.joblib'.format(param,split))\n",
    "    ############## validate set ################\n",
    "    print(\"############## validate set ################\")\n",
    "    y_pred_val = loaded_model.predict(x_val)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "    print('Confusion Matrix')\n",
    "    print('-'*16)\n",
    "    print(cm_val,'\\n')\n",
    "    print('-'*16)\n",
    "\n",
    "    # Calculate score\n",
    "    print(split)\n",
    "    sen_val = sensitivity(cm_val)\n",
    "    spec_val = specificity(cm_val)\n",
    "    acc_val = accuracy(cm_val)\n",
    "    pre_val = precision(cm_val)\n",
    "    f1_score_val = f1(cm_val)\n",
    "\n",
    "    # plot confusion matrix\n",
    "    # plot confusion matrix\n",
    "    class_names = ['benign','malignant']\n",
    "    # Normalize confusion matrix to percentage\n",
    "    cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "    ax.grid(False)\n",
    "    # Add labels\n",
    "    ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "           yticks=np.arange(cm_norm_val.shape[0]),\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "    ax.set_title(\"Validate set\", fontsize=16)\n",
    "    ax.set_ylabel('Actual', fontsize=16)\n",
    "    ax.set_xlabel('Predicted', fontsize=16)\n",
    "    # Add percentage and count values inside plot\n",
    "    thresh = cm_norm_val.max() / 2.\n",
    "    for i in range(cm_norm_val.shape[0]):\n",
    "        for j in range(cm_norm_val.shape[1]):\n",
    "            ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "    # plt.savefig('rf_model/validate_cm_{}.png'.format(split))\n",
    "    # plt.savefig('rf_model/validate_cm_best_param2_{}.png'.format(split))\n",
    "    plt.show()\n",
    "    # ################# test set ##################\n",
    "    label = ['benign','malignant']\n",
    "    print(\"################# test set ##################\")\n",
    "    y_pred_test = loaded_model.predict(x_test)\n",
    "    # Print the predictions\n",
    "    # print(y_pred)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "    print('Confusion Matrix')\n",
    "    print('-'*16)\n",
    "    print(cm_test,'\\n')\n",
    "    print('-'*16)\n",
    "\n",
    "    # Calculate score\n",
    "    print(split)\n",
    "    sen_test = sensitivity(cm_test)\n",
    "    spec_test = specificity(cm_test)\n",
    "    acc_test = accuracy(cm_test)\n",
    "    pre_test = precision(cm_test)\n",
    "    f1_score_test = f1(cm_test)\n",
    "\n",
    "    # plot confusion matrix\n",
    "    # plot confusion matrix\n",
    "    class_names = ['benign','malignant']\n",
    "    # Normalize confusion matrix to percentage\n",
    "    cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "    ax.grid(False)\n",
    "    # Add labels\n",
    "    ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "           yticks=np.arange(cm_norm_test.shape[0]),\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "    ax.set_title(\"Test set\", fontsize=16)\n",
    "    ax.set_ylabel('Actual', fontsize=16)\n",
    "    ax.set_xlabel('Predicted', fontsize=16)\n",
    "    # Add percentage and count values inside plot\n",
    "    thresh = cm_norm_test.max() / 2.\n",
    "    for i in range(cm_norm_test.shape[0]):\n",
    "        for j in range(cm_norm_test.shape[1]):\n",
    "            ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ################## plot ROC curve ########################\n",
    "    fpr1, tpr1, thr1 = roc_curve(y_val,  y_pred_val)\n",
    "    auc1 = roc_auc_score(y_val,  y_pred_val)\n",
    "    fpr2, tpr2, thr2 = roc_curve(y_test,  y_pred_test)\n",
    "    auc2 = roc_auc_score(y_test,  y_pred_test)\n",
    "    gmeans2 = np.sqrt(tpr2 * (1-fpr2))\n",
    "    ix2 = np.argmax(gmeans2)\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"AUC validate: %.4f\" % auc1)\n",
    "    print(\"AUC test: %.4f\" % auc2)\n",
    "    plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc1))\n",
    "    plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc2))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    return cm_val,cm_test,auc1,auc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934cb27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path final test\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "\n",
    "path = \"../Anny_oversampling/DATA/\"\n",
    "train_path = \"{}/train.csv\".format(path)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "print(train_path)\n",
    "# Load the train set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "print(test_path)\n",
    "# Load the test set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[3:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[2]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "# StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30eb283",
   "metadata": {},
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a3455",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split1 = np.array([[51, 1], [2, 37]])\n",
    "test_split2 = np.array([[52, 1], [3, 35]])\n",
    "test_split3 = np.array([[66, 0], [1, 24]])\n",
    "test_split4 = np.array([[71, 1], [0, 19]])\n",
    "test_split5 = np.array([[67, 1], [0, 23]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b686ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_test = test_split1 + test_split2 + test_split3 + test_split4 + test_split5\n",
    "print(overall_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f473b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = overall_test.astype('float') / overall_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(overall_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760effad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_values = []\n",
    "\n",
    "sensitivity_values.append(sen_val_split1)\n",
    "sensitivity_values.append(sen_val_split2)\n",
    "sensitivity_values.append(sen_val_split3)\n",
    "sensitivity_values.append(sen_val_split4)\n",
    "sensitivity_values.append(sen_val_split5)\n",
    "\n",
    "print(sensitivity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_sensitivity = statistics.mean(sensitivity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "sensitivity_sd = statistics.stdev(sensitivity_values)\n",
    "\n",
    "print(\"Mean Sensitivity:\", mean_sensitivity)\n",
    "print(\"Sensitivity Standard Deviation:\", sensitivity_sd)\n",
    "\n",
    "# Print mean sensitivity with standard deviation\n",
    "print(\"Overall Sensitivity: %.2f  %.2f%%\" % (mean_sensitivity, sensitivity_sd))\n",
    "\n",
    "specificity_values = []\n",
    "\n",
    "specificity_values.append(spec_val_split1)\n",
    "specificity_values.append(spec_val_split2)\n",
    "specificity_values.append(spec_val_split3)\n",
    "specificity_values.append(spec_val_split4)\n",
    "specificity_values.append(spec_val_split5)\n",
    "    \n",
    "print(specificity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_specificity = statistics.mean(specificity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "specificity_sd = statistics.stdev(specificity_values)\n",
    "\n",
    "print(\"Mean Specificity:\", mean_specificity)\n",
    "print(\"Specificity Standard Deviation:\", specificity_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall Specificity: %.2f  %.2f%%\" % (mean_specificity, specificity_sd))\n",
    "\n",
    "accuracy_values = []\n",
    "\n",
    "accuracy_values.append(acc_val_split1)\n",
    "accuracy_values.append(acc_val_split2)\n",
    "accuracy_values.append(acc_val_split3)\n",
    "accuracy_values.append(acc_val_split4)\n",
    "accuracy_values.append(acc_val_split5)\n",
    "    \n",
    "print(accuracy_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_accuracy = statistics.mean(accuracy_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "accuracy_sd = statistics.stdev(accuracy_values)\n",
    "\n",
    "print(\"Mean accuracy:\", mean_accuracy)\n",
    "print(\"accuracy Standard Deviation:\", accuracy_sd)\n",
    "\n",
    "# Print mean accuracy with standard deviation\n",
    "print(\"Overall accuracy: %.2f  %.2f%%\" % (mean_accuracy, accuracy_sd))\n",
    "\n",
    "precision_values = []\n",
    "\n",
    "precision_values.append(pre_val_split1)\n",
    "precision_values.append(pre_val_split2)\n",
    "precision_values.append(pre_val_split3)\n",
    "precision_values.append(pre_val_split4)\n",
    "precision_values.append(pre_val_split5)\n",
    "    \n",
    "print(precision_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_precision = statistics.mean(precision_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "precision_sd = statistics.stdev(precision_values)\n",
    "\n",
    "print(\"Mean precision:\", mean_precision)\n",
    "print(\"precision Standard Deviation:\", precision_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall precision: %.2f  %.2f%%\" % (mean_precision, precision_sd))\n",
    "\n",
    "f1_values = []\n",
    "\n",
    "f1_values.append(f1_val_split1)\n",
    "f1_values.append(f1_val_split2)\n",
    "f1_values.append(f1_val_split3)\n",
    "f1_values.append(f1_val_split4)\n",
    "f1_values.append(f1_val_split5)\n",
    "    \n",
    "print(f1_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_f1 = statistics.mean(f1_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "f1_sd = statistics.stdev(f1_values)\n",
    "\n",
    "print(\"Mean f1:\", mean_f1)\n",
    "print(\"f1 Standard Deviation:\", f1_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall f1: %.2f  %.2f%%\" % (mean_f1, f1_sd))\n",
    "\n",
    "auc_values = []\n",
    "\n",
    "auc_values.append(auc_val_split1)\n",
    "auc_values.append(auc_val_split2)\n",
    "auc_values.append(auc_val_split3)\n",
    "auc_values.append(auc_val_split4)\n",
    "auc_values.append(auc_val_split5)\n",
    "    \n",
    "print(auc_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_auc = statistics.mean(auc_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "auc_sd = statistics.stdev(auc_values)\n",
    "\n",
    "print(\"Mean auc:\", mean_auc)\n",
    "print(\"auc Standard Deviation:\", auc_sd)\n",
    "\n",
    "# Print mean auc with standard deviation\n",
    "print(\"Overall auc: %.2f  %.2f%%\" % (mean_auc * 100, auc_sd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa58b7",
   "metadata": {},
   "source": [
    "parameter3 split3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split3'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search logistic regression model\n",
    "\n",
    "# define model\n",
    "#model = LogisticRegression()\n",
    "\n",
    "# define parameter range\n",
    "#param_grid = {'solver' : ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "#              'penalty' : ['l2'],\n",
    "#              'C' : np.logspace(-4, 4, 50),\n",
    "#              'max_iter' : [1000, 2000, 3000, 4000, 5000]}\n",
    "\n",
    "# define search\n",
    "#search = GridSearchCV(model, param_grid, scoring='accuracy', n_jobs=-1, error_score='raise', cv=5)\n",
    "\n",
    "# execute search\n",
    "#result = search.fit(x_train, y_train)\n",
    "\n",
    "# summarize result\n",
    "#print('Best Score: %s' % result.best_score_)\n",
    "#print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the individual models\n",
    "model = LogisticRegression(C=2.559547922699533, max_iter=1000, penalty='l2', solver='lbfgs')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param3/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d4390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split3 = sensitivity(cm_val)\n",
    "spec_val_split3 = specificity(cm_val)\n",
    "acc_val_split3 = accuracy(cm_val)\n",
    "pre_val_split3 = precision(cm_val)\n",
    "f1_val_split3 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split3 = sensitivity(cm_test)\n",
    "spec_test_split3 = specificity(cm_test)\n",
    "acc_test_split3 = accuracy(cm_test)\n",
    "pre_test_split3 = precision(cm_test)\n",
    "f1_test_split3 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split3 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split3 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split3)\n",
    "print(\"AUC test: %.4f\" % auc_test_split3)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split3))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split3))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44019e87",
   "metadata": {},
   "source": [
    "split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split1'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param3/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split1 = sensitivity(cm_val)\n",
    "spec_val_split1 = specificity(cm_val)\n",
    "acc_val_split1 = accuracy(cm_val)\n",
    "pre_val_split1 = precision(cm_val)\n",
    "f1_val_split1 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split1 = sensitivity(cm_test)\n",
    "spec_test_split1 = specificity(cm_test)\n",
    "acc_test_split1 = accuracy(cm_test)\n",
    "pre_test_split1 = precision(cm_test)\n",
    "f1_test_split1 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split1 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split1 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split1)\n",
    "print(\"AUC test: %.4f\" % auc_test_split1)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split1))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split1))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd0d1ce",
   "metadata": {},
   "source": [
    "split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efed21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split2'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param3/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split2 = sensitivity(cm_val)\n",
    "spec_val_split2 = specificity(cm_val)\n",
    "acc_val_split2 = accuracy(cm_val)\n",
    "pre_val_split2 = precision(cm_val)\n",
    "f1_val_split2 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split2 = sensitivity(cm_test)\n",
    "spec_test_split2 = specificity(cm_test)\n",
    "acc_test_split2 = accuracy(cm_test)\n",
    "pre_test_split2 = precision(cm_test)\n",
    "f1_test_split2 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split2 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split2 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split2)\n",
    "print(\"AUC test: %.4f\" % auc_test_split2)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split2))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split2))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42d762",
   "metadata": {},
   "source": [
    "split4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split4'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param3/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5828d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split4 = sensitivity(cm_val)\n",
    "spec_val_split4 = specificity(cm_val)\n",
    "acc_val_split4 = accuracy(cm_val)\n",
    "pre_val_split4 = precision(cm_val)\n",
    "f1_val_split4 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split4 = sensitivity(cm_test)\n",
    "spec_test_split4 = specificity(cm_test)\n",
    "acc_test_split4 = accuracy(cm_test)\n",
    "pre_test_split4 = precision(cm_test)\n",
    "f1_test_split4 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split4 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split4 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split4)\n",
    "print(\"AUC test: %.4f\" % auc_test_split4)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split4))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split4))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad36ddb6",
   "metadata": {},
   "source": [
    "split5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split5'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param3/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split5 = sensitivity(cm_val)\n",
    "spec_val_split5 = specificity(cm_val)\n",
    "acc_val_split5 = accuracy(cm_val)\n",
    "pre_val_split5 = precision(cm_val)\n",
    "f1_val_split5 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split5 = sensitivity(cm_test)\n",
    "spec_test_split5 = specificity(cm_test)\n",
    "acc_test_split5 = accuracy(cm_test)\n",
    "pre_test_split5 = precision(cm_test)\n",
    "f1_test_split5 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split5 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split5 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split5)\n",
    "print(\"AUC test: %.4f\" % auc_test_split5)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split5))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split5))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd75ee6d",
   "metadata": {},
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff594fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_values = []\n",
    "\n",
    "sensitivity_values.append(sen_val_split1)\n",
    "sensitivity_values.append(sen_val_split2)\n",
    "sensitivity_values.append(sen_val_split3)\n",
    "sensitivity_values.append(sen_val_split4)\n",
    "sensitivity_values.append(sen_val_split5)\n",
    "\n",
    "print(sensitivity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_sensitivity = statistics.mean(sensitivity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "sensitivity_sd = statistics.stdev(sensitivity_values)\n",
    "\n",
    "print(\"Mean Sensitivity:\", mean_sensitivity)\n",
    "print(\"Sensitivity Standard Deviation:\", sensitivity_sd)\n",
    "\n",
    "# Print mean sensitivity with standard deviation\n",
    "print(\"Overall Sensitivity: %.2f  %.2f%%\" % (mean_sensitivity, sensitivity_sd))\n",
    "\n",
    "specificity_values = []\n",
    "\n",
    "specificity_values.append(spec_val_split1)\n",
    "specificity_values.append(spec_val_split2)\n",
    "specificity_values.append(spec_val_split3)\n",
    "specificity_values.append(spec_val_split4)\n",
    "specificity_values.append(spec_val_split5)\n",
    "    \n",
    "print(specificity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_specificity = statistics.mean(specificity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "specificity_sd = statistics.stdev(specificity_values)\n",
    "\n",
    "print(\"Mean Specificity:\", mean_specificity)\n",
    "print(\"Specificity Standard Deviation:\", specificity_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall Specificity: %.2f  %.2f%%\" % (mean_specificity, specificity_sd))\n",
    "\n",
    "accuracy_values = []\n",
    "\n",
    "accuracy_values.append(acc_val_split1)\n",
    "accuracy_values.append(acc_val_split2)\n",
    "accuracy_values.append(acc_val_split3)\n",
    "accuracy_values.append(acc_val_split4)\n",
    "accuracy_values.append(acc_val_split5)\n",
    "    \n",
    "print(accuracy_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_accuracy = statistics.mean(accuracy_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "accuracy_sd = statistics.stdev(accuracy_values)\n",
    "\n",
    "print(\"Mean accuracy:\", mean_accuracy)\n",
    "print(\"accuracy Standard Deviation:\", accuracy_sd)\n",
    "\n",
    "# Print mean accuracy with standard deviation\n",
    "print(\"Overall accuracy: %.2f  %.2f%%\" % (mean_accuracy, accuracy_sd))\n",
    "\n",
    "precision_values = []\n",
    "\n",
    "precision_values.append(pre_val_split1)\n",
    "precision_values.append(pre_val_split2)\n",
    "precision_values.append(pre_val_split3)\n",
    "precision_values.append(pre_val_split4)\n",
    "precision_values.append(pre_val_split5)\n",
    "    \n",
    "print(precision_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_precision = statistics.mean(precision_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "precision_sd = statistics.stdev(precision_values)\n",
    "\n",
    "print(\"Mean precision:\", mean_precision)\n",
    "print(\"precision Standard Deviation:\", precision_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall precision: %.2f  %.2f%%\" % (mean_precision, precision_sd))\n",
    "\n",
    "f1_values = []\n",
    "\n",
    "f1_values.append(f1_val_split1)\n",
    "f1_values.append(f1_val_split2)\n",
    "f1_values.append(f1_val_split3)\n",
    "f1_values.append(f1_val_split4)\n",
    "f1_values.append(f1_val_split5)\n",
    "    \n",
    "print(f1_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_f1 = statistics.mean(f1_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "f1_sd = statistics.stdev(f1_values)\n",
    "\n",
    "print(\"Mean f1:\", mean_f1)\n",
    "print(\"f1 Standard Deviation:\", f1_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall f1: %.2f  %.2f%%\" % (mean_f1, f1_sd))\n",
    "\n",
    "auc_values = []\n",
    "\n",
    "auc_values.append(auc_val_split1)\n",
    "auc_values.append(auc_val_split2)\n",
    "auc_values.append(auc_val_split3)\n",
    "auc_values.append(auc_val_split4)\n",
    "auc_values.append(auc_val_split5)\n",
    "    \n",
    "print(auc_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_auc = statistics.mean(auc_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "auc_sd = statistics.stdev(auc_values)\n",
    "\n",
    "print(\"Mean auc:\", mean_auc)\n",
    "print(\"auc Standard Deviation:\", auc_sd)\n",
    "\n",
    "# Print mean auc with standard deviation\n",
    "print(\"Overall auc: %.2f  %.2f%%\" % (mean_auc * 100, auc_sd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25aa45c",
   "metadata": {},
   "source": [
    "parameter4 split4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d806482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split4'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search logistic regression model\n",
    "\n",
    "# define model\n",
    "#model = LogisticRegression()\n",
    "\n",
    "# define parameter range\n",
    "#param_grid = {'solver' : ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "#              'penalty' : ['l2'],\n",
    "#              'C' : np.logspace(-4, 4, 50),\n",
    "#              'max_iter' : [1000, 2000, 3000, 4000, 5000]}\n",
    "\n",
    "# define search\n",
    "#search = GridSearchCV(model, param_grid, scoring='accuracy', n_jobs=-1, error_score='raise', cv=5)\n",
    "\n",
    "# execute search\n",
    "#result = search.fit(x_train, y_train)\n",
    "\n",
    "# summarize result\n",
    "#print('Best Score: %s' % result.best_score_)\n",
    "#print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76038019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the individual models\n",
    "model = LogisticRegression(C=2222.996482526191, max_iter=1000, penalty='l2', solver='lbfgs')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param4/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split4 = sensitivity(cm_val)\n",
    "spec_val_split4 = specificity(cm_val)\n",
    "acc_val_split4 = accuracy(cm_val)\n",
    "pre_val_split4 = precision(cm_val)\n",
    "f1_val_split4 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split4 = sensitivity(cm_test)\n",
    "spec_test_split4 = specificity(cm_test)\n",
    "acc_test_split4 = accuracy(cm_test)\n",
    "pre_test_split4 = precision(cm_test)\n",
    "f1_test_split4 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split4 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split4 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split4)\n",
    "print(\"AUC test: %.4f\" % auc_test_split4)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split4))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split4))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ff771",
   "metadata": {},
   "source": [
    "split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4008ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split1'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param4/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split1 = sensitivity(cm_val)\n",
    "spec_val_split1 = specificity(cm_val)\n",
    "acc_val_split1 = accuracy(cm_val)\n",
    "pre_val_split1 = precision(cm_val)\n",
    "f1_val_split1 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split1 = sensitivity(cm_test)\n",
    "spec_test_split1 = specificity(cm_test)\n",
    "acc_test_split1 = accuracy(cm_test)\n",
    "pre_test_split1 = precision(cm_test)\n",
    "f1_test_split1 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split1 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split1 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split1)\n",
    "print(\"AUC test: %.4f\" % auc_test_split1)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split1))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split1))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63b6ee",
   "metadata": {},
   "source": [
    "split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbd107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split2'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param4/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6239d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split2 = sensitivity(cm_val)\n",
    "spec_val_split2 = specificity(cm_val)\n",
    "acc_val_split2 = accuracy(cm_val)\n",
    "pre_val_split2 = precision(cm_val)\n",
    "f1_val_split2 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split2 = sensitivity(cm_test)\n",
    "spec_test_split2 = specificity(cm_test)\n",
    "acc_test_split2 = accuracy(cm_test)\n",
    "pre_test_split2 = precision(cm_test)\n",
    "f1_test_split2 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split2 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split2 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split2)\n",
    "print(\"AUC test: %.4f\" % auc_test_split2)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split2))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split2))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdcaaec",
   "metadata": {},
   "source": [
    "split3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d8ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split3'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param4/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split3 = sensitivity(cm_val)\n",
    "spec_val_split3 = specificity(cm_val)\n",
    "acc_val_split3 = accuracy(cm_val)\n",
    "pre_val_split3 = precision(cm_val)\n",
    "f1_val_split3 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split3 = sensitivity(cm_test)\n",
    "spec_test_split3 = specificity(cm_test)\n",
    "acc_test_split3 = accuracy(cm_test)\n",
    "pre_test_split3 = precision(cm_test)\n",
    "f1_test_split3 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split3 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split3 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split3)\n",
    "print(\"AUC test: %.4f\" % auc_test_split3)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split3))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split3))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07285a29",
   "metadata": {},
   "source": [
    "split5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc90a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split5'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param4/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val_split5 = sensitivity(cm_val)\n",
    "spec_val_split5 = specificity(cm_val)\n",
    "acc_val_split5 = accuracy(cm_val)\n",
    "pre_val_split5 = precision(cm_val)\n",
    "f1_val_split5 = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split5 = sensitivity(cm_test)\n",
    "spec_test_split5 = specificity(cm_test)\n",
    "acc_test_split5 = accuracy(cm_test)\n",
    "pre_test_split5 = precision(cm_test)\n",
    "f1_test_split5 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val_split5 = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split5 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val_split5)\n",
    "print(\"AUC test: %.4f\" % auc_test_split5)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val_split5))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split5))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf624e3",
   "metadata": {},
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_values = []\n",
    "\n",
    "sensitivity_values.append(sen_val_split1)\n",
    "sensitivity_values.append(sen_val_split2)\n",
    "sensitivity_values.append(sen_val_split3)\n",
    "sensitivity_values.append(sen_val_split4)\n",
    "sensitivity_values.append(sen_val_split5)\n",
    "\n",
    "print(sensitivity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_sensitivity = statistics.mean(sensitivity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "sensitivity_sd = statistics.stdev(sensitivity_values)\n",
    "\n",
    "print(\"Mean Sensitivity:\", mean_sensitivity)\n",
    "print(\"Sensitivity Standard Deviation:\", sensitivity_sd)\n",
    "\n",
    "# Print mean sensitivity with standard deviation\n",
    "print(\"Overall Sensitivity: %.2f  %.2f%%\" % (mean_sensitivity, sensitivity_sd))\n",
    "\n",
    "specificity_values = []\n",
    "\n",
    "specificity_values.append(spec_val_split1)\n",
    "specificity_values.append(spec_val_split2)\n",
    "specificity_values.append(spec_val_split3)\n",
    "specificity_values.append(spec_val_split4)\n",
    "specificity_values.append(spec_val_split5)\n",
    "    \n",
    "print(specificity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_specificity = statistics.mean(specificity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "specificity_sd = statistics.stdev(specificity_values)\n",
    "\n",
    "print(\"Mean Specificity:\", mean_specificity)\n",
    "print(\"Specificity Standard Deviation:\", specificity_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall Specificity: %.2f  %.2f%%\" % (mean_specificity, specificity_sd))\n",
    "\n",
    "accuracy_values = []\n",
    "\n",
    "accuracy_values.append(acc_val_split1)\n",
    "accuracy_values.append(acc_val_split2)\n",
    "accuracy_values.append(acc_val_split3)\n",
    "accuracy_values.append(acc_val_split4)\n",
    "accuracy_values.append(acc_val_split5)\n",
    "    \n",
    "print(accuracy_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_accuracy = statistics.mean(accuracy_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "accuracy_sd = statistics.stdev(accuracy_values)\n",
    "\n",
    "print(\"Mean accuracy:\", mean_accuracy)\n",
    "print(\"accuracy Standard Deviation:\", accuracy_sd)\n",
    "\n",
    "# Print mean accuracy with standard deviation\n",
    "print(\"Overall accuracy: %.2f  %.2f%%\" % (mean_accuracy, accuracy_sd))\n",
    "\n",
    "precision_values = []\n",
    "\n",
    "precision_values.append(pre_val_split1)\n",
    "precision_values.append(pre_val_split2)\n",
    "precision_values.append(pre_val_split3)\n",
    "precision_values.append(pre_val_split4)\n",
    "precision_values.append(pre_val_split5)\n",
    "    \n",
    "print(precision_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_precision = statistics.mean(precision_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "precision_sd = statistics.stdev(precision_values)\n",
    "\n",
    "print(\"Mean precision:\", mean_precision)\n",
    "print(\"precision Standard Deviation:\", precision_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall precision: %.2f  %.2f%%\" % (mean_precision, precision_sd))\n",
    "\n",
    "f1_values = []\n",
    "\n",
    "f1_values.append(f1_val_split1)\n",
    "f1_values.append(f1_val_split2)\n",
    "f1_values.append(f1_val_split3)\n",
    "f1_values.append(f1_val_split4)\n",
    "f1_values.append(f1_val_split5)\n",
    "    \n",
    "print(f1_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_f1 = statistics.mean(f1_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "f1_sd = statistics.stdev(f1_values)\n",
    "\n",
    "print(\"Mean f1:\", mean_f1)\n",
    "print(\"f1 Standard Deviation:\", f1_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall f1: %.2f  %.2f%%\" % (mean_f1, f1_sd))\n",
    "\n",
    "auc_values = []\n",
    "\n",
    "auc_values.append(auc_val_split1)\n",
    "auc_values.append(auc_val_split2)\n",
    "auc_values.append(auc_val_split3)\n",
    "auc_values.append(auc_val_split4)\n",
    "auc_values.append(auc_val_split5)\n",
    "    \n",
    "print(auc_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_auc = statistics.mean(auc_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "auc_sd = statistics.stdev(auc_values)\n",
    "\n",
    "print(\"Mean auc:\", mean_auc)\n",
    "print(\"auc Standard Deviation:\", auc_sd)\n",
    "\n",
    "# Print mean auc with standard deviation\n",
    "print(\"Overall auc: %.2f  %.2f%%\" % (mean_auc * 100, auc_sd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04738889",
   "metadata": {},
   "source": [
    "parameter5 split5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f73b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split5'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search logistic regression model\n",
    "\n",
    "# define model\n",
    "#model = LogisticRegression()\n",
    "\n",
    "# define parameter range\n",
    "#param_grid = {'solver' : ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "#              'penalty' : ['l2'],\n",
    "#              'C' : np.logspace(-4, 4, 50),\n",
    "#              'max_iter' : [1000, 2000, 3000, 4000, 5000]}\n",
    "\n",
    "# define search\n",
    "#search = GridSearchCV(model, param_grid, scoring='accuracy', n_jobs=-1, error_score='raise', cv=5)\n",
    "\n",
    "# execute search\n",
    "#result = search.fit(x_train, y_train)\n",
    "\n",
    "# summarize result\n",
    "#print('Best Score: %s' % result.best_score_)\n",
    "#print('Best Hyperparameters: %s' % result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab39974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the individual models\n",
    "model = LogisticRegression(C=1.7575106248547894, max_iter=1000, penalty='l2', solver='saga')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param5/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a29e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split5 = sensitivity(cm_test)\n",
    "spec_test_split5 = specificity(cm_test)\n",
    "acc_test_split5 = accuracy(cm_test)\n",
    "pre_test_split5 = precision(cm_test)\n",
    "f1_test_split5 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split5 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split5)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split5))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70fcdbf",
   "metadata": {},
   "source": [
    "split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a41767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split1'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param5/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split1 = sensitivity(cm_test)\n",
    "spec_test_split1 = specificity(cm_test)\n",
    "acc_test_split1 = accuracy(cm_test)\n",
    "pre_test_split1 = precision(cm_test)\n",
    "f1_test_split1 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split1 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split1)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split1))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd87c5",
   "metadata": {},
   "source": [
    "split2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ba437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split2'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param5/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split2 = sensitivity(cm_test)\n",
    "spec_test_split2 = specificity(cm_test)\n",
    "acc_test_split2 = accuracy(cm_test)\n",
    "pre_test_split2 = precision(cm_test)\n",
    "f1_test_split2 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split2 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split2)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split2))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313d88f",
   "metadata": {},
   "source": [
    "split3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b3ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split3'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param5/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e085dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val = model.predict(x_val)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split3 = sensitivity(cm_test)\n",
    "spec_test_split3 = specificity(cm_test)\n",
    "acc_test_split3 = accuracy(cm_test)\n",
    "pre_test_split3 = precision(cm_test)\n",
    "f1_test_split3 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\")\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "fpr1, tpr1, thr1 = roc_curve(y_val, y_pred_val)\n",
    "auc_val = roc_auc_score(y_val, y_pred_val)\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test)\n",
    "auc_test_split3 = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.4f\" % auc_val)\n",
    "print(\"AUC test: %.4f\" % auc_test_split3)\n",
    "\n",
    "# plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc),color='red')\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc_val))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc_test_split3))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38ca11",
   "metadata": {},
   "source": [
    "split4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fdfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting split and path\n",
    "split = 'split4'\n",
    "path = \"../kitt/DATA_progress3/\"\n",
    "train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "test_path =  \"{}/test.csv\".format(path)\n",
    "val_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "# Load the training set\n",
    "train_data = pd.read_csv(train_path)\n",
    "print(train_data.shape)\n",
    "x_train = train_data[train_data.columns[2:]]\n",
    "print(x_train.shape)\n",
    "y_train = train_data[train_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_train = np.array(le.fit_transform(y_train))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the testing set\n",
    "test_data = pd.read_csv(test_path)\n",
    "print(test_data.shape)\n",
    "x_test = test_data[test_data.columns[2:]]\n",
    "print(x_test.shape)\n",
    "y_test = test_data[test_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_test = np.array(le.fit_transform(y_test))\n",
    "print(le.classes_)\n",
    "\n",
    "# Load the validation set\n",
    "val_data = pd.read_csv(val_path)\n",
    "print(val_data.shape)\n",
    "x_val = val_data[val_data.columns[2:]]\n",
    "print(x_val.shape)\n",
    "y_val = val_data[val_data.columns[1]]\n",
    "le = LabelEncoder()\n",
    "y_val = np.array(le.fit_transform(y_val))\n",
    "print(le.classes_)\n",
    "\n",
    "# check balance in this split\n",
    "print(split)\n",
    "print(\"train positive:    \" + str(sum(y_train)))\n",
    "print(\"validate positive: \" + str(sum(y_val)))\n",
    "\n",
    "x_train = x_train.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "x_val = x_val.to_numpy()\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "x_val = sc.transform(x_val)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "model_path = \"../logisticregression/models/param5/lr_model_{}.pkl\".format(split)\n",
    "\n",
    "# Save the trained model to disk\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Load the saved model from disk\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb4f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    "    \n",
    "def thresholding(y_val, y_pred_val):\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_pred_val[:, 1])\n",
    "    # get the best threshold\n",
    "    J = tpr - fpr\n",
    "    ix = np.argmax(J)\n",
    "    best_thresh = thresholds[ix]\n",
    "    print('Best Threshold=%f' % (best_thresh))\n",
    "    print('FPR: %.4f\\nTPR: %.4f' %(fpr[ix], tpr[ix]))\n",
    "    y_pred_val_new = to_labels(y_pred_val[:, 1], best_thresh)\n",
    "    return y_pred_val_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8960bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_split(split1,split2,split3,split4,split5):\n",
    "    fpr1, tpr1, thr1 = metrics.roc_curve(y_val,  y_pred_val)\n",
    "    auc1 = metrics.roc_auc_score(y_val,  y_pred_val)*100\n",
    "    \n",
    "    fpr2, tpr2, thr2 = metrics.roc_curve(y_test,  y_pred_test)\n",
    "    auc2 = metrics.roc_auc_score(y_test,  y_pred_test)*100\n",
    "    \n",
    "    fpr3, tpr3, thr3 = metrics.roc_curve(y_test,  y_pred_test)\n",
    "    auc3 = metrics.roc_auc_score(y_test,  y_pred_test)*100\n",
    "    \n",
    "    fpr4, tpr4, thr4 = metrics.roc_curve(y_test,  y_pred_test)\n",
    "    auc4 = metrics.roc_auc_score(y_test,  y_pred_test)*100\n",
    "    \n",
    "    fpr5, tpr5, thr5 = metrics.roc_curve(y_test,  y_pred_test)\n",
    "    auc5 = metrics.roc_auc_score(y_test,  y_pred_test)*100\n",
    "    \n",
    "    # gmeans2 = np.sqrt(tpr2 * (1-fpr2))\n",
    "    # ix2 = np.argmax(gmeans2)\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"AUC split1: %.2f%%\" % auc1)\n",
    "    print(\"AUC split2: %.2f%%\" % auc2)\n",
    "    print(\"AUC split3: %.2f%%\" % auc1)\n",
    "    print(\"AUC split4: %.2f%%\" % auc2)\n",
    "    print(\"AUC split5: %.2f%%\" % auc1)\n",
    "    \n",
    "    plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.2f%%\" % auc1))\n",
    "    plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.2f%%\" % auc2))\n",
    "    plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.2f%%\" % auc3))\n",
    "    plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.2f%%\" % auc4))\n",
    "    plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.2f%%\" % auc5))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split)\n",
    "############## validate set ################\n",
    "print(\"############## validate set ################\")\n",
    "y_pred_val_raw = model.predict_proba(x_val)\n",
    "y_pred_val = thresholding(y_val, y_pred_val_raw)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_val,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_val = sensitivity(cm_val)\n",
    "spec_val = specificity(cm_val)\n",
    "acc_val = accuracy(cm_val)\n",
    "pre_val = precision(cm_val)\n",
    "f1_val = f1(cm_val)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "       yticks=np.arange(cm_norm_val.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Validate set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_val.max() / 2.\n",
    "for i in range(cm_norm_val.shape[0]):\n",
    "    for j in range(cm_norm_val.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_val[i, j] > thresh else \"black\", fontsize=20)\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################# test set ##################\n",
    "print(\"################# test set ##################\")\n",
    "y_pred_test_raw = model.predict_proba(x_test)\n",
    "y_pred_test = thresholding(y_test, y_pred_test_raw)\n",
    "\n",
    "# Confusion matrix\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "print('Confusion Matrix')\n",
    "print('-'*16)\n",
    "print(cm_test,'\\n')\n",
    "print('-'*16)\n",
    "\n",
    "# Calculate score\n",
    "sen_test_split4 = sensitivity(cm_test)\n",
    "spec_test_split4 = specificity(cm_test)\n",
    "acc_test_split4 = accuracy(cm_test)\n",
    "pre_test_split4 = precision(cm_test)\n",
    "f1_test_split4 = f1(cm_test)\n",
    "\n",
    "# plot confusion matrix\n",
    "class_names = ['benign','malignant']\n",
    "# Normalize confusion matrix to percentage\n",
    "cm_norm_test = cm_test.astype('float') / cm_test.sum(axis=1)[:, np.newaxis]\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm_norm_test, interpolation='nearest', cmap=\"rocket_r\")\n",
    "ax.grid(False)\n",
    "# Add labels\n",
    "ax.set(xticks=np.arange(cm_norm_test.shape[1]),\n",
    "       yticks=np.arange(cm_norm_test.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names)\n",
    "ax.set_title(\"Test set\", fontsize=16)\n",
    "ax.set_ylabel('Actual', fontsize=16)\n",
    "ax.set_xlabel('Predicted', fontsize=16)\n",
    "# Add percentage and count values inside plot\n",
    "thresh = cm_norm_test.max() / 2.\n",
    "for i in range(cm_norm_test.shape[0]):\n",
    "    for j in range(cm_norm_test.shape[1]):\n",
    "        ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_test[i,j], cm_norm_test[i, j]*100),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm_norm_test[i, j] > thresh else \"black\", fontsize=20)\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "################## plot ROC curve ########################\n",
    "pr1, tpr1, thr1 = roc_curve(y_val, y_pred_val_raw[:,1])\n",
    "auc1 = roc_auc_score(y_val, y_pred_val_raw[:,1])*100\n",
    "fpr2, tpr2, thr2 = roc_curve(y_test, y_pred_test_raw[:,1])\n",
    "auc2 = roc_auc_score(y_test, y_pred_test_raw[:,1])*100\n",
    "gmeans2 = np.sqrt(tpr2 * (1-fpr2))\n",
    "ix2 = np.argmax(gmeans2)\n",
    "print(\"---------------------------------------\")\n",
    "print(\"AUC validate: %.2f%%\" % auc1)\n",
    "print(\"AUC test: %.2f%%\" % auc2)\n",
    "plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.2f%%\" % auc1))\n",
    "plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.2f%%\" % auc2))\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0969b7",
   "metadata": {},
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ac61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_values = []\n",
    "\n",
    "sensitivity_values.append(sen_val_split1)\n",
    "sensitivity_values.append(sen_val_split2)\n",
    "sensitivity_values.append(sen_val_split3)\n",
    "sensitivity_values.append(sen_val_split4)\n",
    "sensitivity_values.append(sen_val_split5)\n",
    "\n",
    "print(sensitivity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_sensitivity = statistics.mean(sensitivity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "sensitivity_sd = statistics.stdev(sensitivity_values)\n",
    "\n",
    "print(\"Mean Sensitivity:\", mean_sensitivity)\n",
    "print(\"Sensitivity Standard Deviation:\", sensitivity_sd)\n",
    "\n",
    "# Print mean sensitivity with standard deviation\n",
    "print(\"Overall Sensitivity: %.2f  %.2f%%\" % (mean_sensitivity, sensitivity_sd))\n",
    "\n",
    "specificity_values = []\n",
    "\n",
    "specificity_values.append(spec_val_split1)\n",
    "specificity_values.append(spec_val_split2)\n",
    "specificity_values.append(spec_val_split3)\n",
    "specificity_values.append(spec_val_split4)\n",
    "specificity_values.append(spec_val_split5)\n",
    "    \n",
    "print(specificity_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_specificity = statistics.mean(specificity_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "specificity_sd = statistics.stdev(specificity_values)\n",
    "\n",
    "print(\"Mean Specificity:\", mean_specificity)\n",
    "print(\"Specificity Standard Deviation:\", specificity_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall Specificity: %.2f  %.2f%%\" % (mean_specificity, specificity_sd))\n",
    "\n",
    "accuracy_values = []\n",
    "\n",
    "accuracy_values.append(acc_val_split1)\n",
    "accuracy_values.append(acc_val_split2)\n",
    "accuracy_values.append(acc_val_split3)\n",
    "accuracy_values.append(acc_val_split4)\n",
    "accuracy_values.append(acc_val_split5)\n",
    "    \n",
    "print(accuracy_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_accuracy = statistics.mean(accuracy_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "accuracy_sd = statistics.stdev(accuracy_values)\n",
    "\n",
    "print(\"Mean accuracy:\", mean_accuracy)\n",
    "print(\"accuracy Standard Deviation:\", accuracy_sd)\n",
    "\n",
    "# Print mean accuracy with standard deviation\n",
    "print(\"Overall accuracy: %.2f  %.2f%%\" % (mean_accuracy, accuracy_sd))\n",
    "\n",
    "precision_values = []\n",
    "\n",
    "precision_values.append(pre_val_split1)\n",
    "precision_values.append(pre_val_split2)\n",
    "precision_values.append(pre_val_split3)\n",
    "precision_values.append(pre_val_split4)\n",
    "precision_values.append(pre_val_split5)\n",
    "    \n",
    "print(precision_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_precision = statistics.mean(precision_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "precision_sd = statistics.stdev(precision_values)\n",
    "\n",
    "print(\"Mean precision:\", mean_precision)\n",
    "print(\"precision Standard Deviation:\", precision_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall precision: %.2f  %.2f%%\" % (mean_precision, precision_sd))\n",
    "\n",
    "f1_values = []\n",
    "\n",
    "f1_values.append(f1_val_split1)\n",
    "f1_values.append(f1_val_split2)\n",
    "f1_values.append(f1_val_split3)\n",
    "f1_values.append(f1_val_split4)\n",
    "f1_values.append(f1_val_split5)\n",
    "    \n",
    "print(f1_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_f1 = statistics.mean(f1_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "f1_sd = statistics.stdev(f1_values)\n",
    "\n",
    "print(\"Mean f1:\", mean_f1)\n",
    "print(\"f1 Standard Deviation:\", f1_sd)\n",
    "\n",
    "# Print mean specificity with standard deviation\n",
    "print(\"Overall f1: %.2f  %.2f%%\" % (mean_f1, f1_sd))\n",
    "\n",
    "auc_values = []\n",
    "\n",
    "auc_values.append(auc_val_split1)\n",
    "auc_values.append(auc_val_split2)\n",
    "auc_values.append(auc_val_split3)\n",
    "auc_values.append(auc_val_split4)\n",
    "auc_values.append(auc_val_split5)\n",
    "    \n",
    "print(auc_values)\n",
    "\n",
    "# calculating the mean of sample set\n",
    "mean_auc = statistics.mean(auc_values)\n",
    "# Calculate standard deviation of sensitivity\n",
    "auc_sd = statistics.stdev(auc_values)\n",
    "\n",
    "print(\"Mean auc:\", mean_auc)\n",
    "print(\"auc Standard Deviation:\", auc_sd)\n",
    "\n",
    "# Print mean auc with standard deviation\n",
    "print(\"Overall auc: %.2f  %.2f%%\" % (mean_auc * 100, auc_sd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "23bf0724a01b6ea9814e66f76182ea78c0ee849a72ca257c0e116bf83bb4960a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

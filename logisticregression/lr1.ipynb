{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import statistics\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tn, fp, fn, tp = each_index(cm)\n",
    "def each_index(metrix):\n",
    "    TN = metrix[0][0]\n",
    "    FP = metrix[0][1]\n",
    "    FN = metrix[1][0]\n",
    "    TP = metrix[1][1]\n",
    "    return TN, FP, FN, TP\n",
    "def sensitivity(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate sensitivity\n",
    "    sensitivity = TP / (TP + FN) * 100\n",
    "    print(\"Sensitivity: %.2f%%\" % sensitivity)\n",
    "    return sensitivity\n",
    "def specificity(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate specificity\n",
    "    specificity = TN / (TN + FP) * 100\n",
    "    print(\"Specificity: %.2f%%\" % specificity)\n",
    "    return specificity\n",
    "def accuracy(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate accuracy\n",
    "    accuracy = ((TP + TN) / (TP + TN + FP + FN)) *100\n",
    "    print(\"Accuracy: %.2f%%\" % accuracy)\n",
    "    return accuracy\n",
    "def precision(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate accuracy\n",
    "    precision = (TP / (TP + FP)) *100\n",
    "    print(\"Precision: %.2f%%\" % precision)\n",
    "    return precision\n",
    "def f1(metrix):\n",
    "    TN, FP, FN, TP = each_index(metrix)\n",
    "    # Calculate f1\n",
    "    recall = sensitivity(metrix)\n",
    "    pre = precision(metrix)\n",
    "    f1 = ((2*pre*recall) / (pre+recall))\n",
    "    print(\"F1: %.2f%%\" % f1)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(split):\n",
    "    # setting split and path\n",
    "    path = \"../kitt/DATA_progress3_new/\"\n",
    "    train_path = \"{}/{}/train/train_oversampling.csv\".format(path,split)\n",
    "    test_path =  \"{}/test.csv\".format(path)\n",
    "    valid_path = \"{}/{}/val/val.csv\".format(path,split)\n",
    "\n",
    "    print(train_path)\n",
    "    # Load the train set\n",
    "    train_data = pd.read_csv(train_path)\n",
    "    print(train_data.shape)\n",
    "    x_train = train_data[train_data.columns[2:]]\n",
    "    print(x_train.shape)\n",
    "    y_train = train_data[train_data.columns[1]]\n",
    "    le = LabelEncoder()\n",
    "    y_train = np.array(le.fit_transform(y_train))\n",
    "    print(le.classes_)\n",
    "    print(test_path)\n",
    "    # Load the test set\n",
    "    test_data = pd.read_csv(test_path)\n",
    "    print(test_data.shape)\n",
    "    x_test = test_data[test_data.columns[2:]]\n",
    "    print(x_test.shape)\n",
    "    y_test = test_data[test_data.columns[1]]\n",
    "    le = LabelEncoder()\n",
    "    y_test = np.array(le.fit_transform(y_test))\n",
    "    print(le.classes_)\n",
    "\n",
    "    print(valid_path)\n",
    "    # Load the validation set\n",
    "    val_data = pd.read_csv(valid_path)\n",
    "    print(val_data.shape)\n",
    "    x_val = val_data[val_data.columns[2:]]\n",
    "    print(x_val.shape)\n",
    "    y_val = val_data[val_data.columns[1]]\n",
    "    le = LabelEncoder()\n",
    "    y_val = np.array(le.fit_transform(y_val))\n",
    "    print(le.classes_)\n",
    "\n",
    "    # StandardScaler\n",
    "    sc = StandardScaler()\n",
    "    x_train = x_train.to_numpy()\n",
    "    x_test = x_test.to_numpy()\n",
    "    x_val = x_val.to_numpy()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.transform(x_test)\n",
    "    x_val = sc.transform(x_val)\n",
    "    \n",
    "    return x_train,y_train,x_test,y_test,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score_all(cm_val):\n",
    "    sen_val = sensitivity(cm_val)\n",
    "    spec_val = specificity(cm_val)\n",
    "    acc_val = accuracy(cm_val)\n",
    "    pre_val = precision(cm_val)\n",
    "    f1_score_val = f1(cm_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(cm_val, save=False,which=\"-\"):\n",
    "    class_names = ['benign','malignant']\n",
    "    # Normalize confusion matrix to percentage\n",
    "    cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "    ax.grid(False)\n",
    "    # Add labels\n",
    "    ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "           yticks=np.arange(cm_norm_val.shape[0]),\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "    ax.set_title(which, fontsize=16)\n",
    "    ax.set_ylabel('Actual', fontsize=16)\n",
    "    ax.set_xlabel('Predicted', fontsize=16)\n",
    "    # Add percentage and count values inside plot\n",
    "    thresh = cm_norm_val.max() / 2.\n",
    "    for i in range(cm_norm_val.shape[0]):\n",
    "        for j in range(cm_norm_val.shape[1]):\n",
    "            ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm_norm_val[i, j] > thresh else \"black\",\n",
    "                   fontsize=20)\n",
    "    if save is not False:\n",
    "        save_path = '{}_{}.png'.format(split)\n",
    "        plt.savefig(save_path)\n",
    "        print(\"Save fig at {}\".format(save_path))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC():\n",
    "    fpr1, tpr1, thr1 = metrics.roc_curve(y_val,  y_pred_val)\n",
    "    auc1 = metrics.roc_auc_score(y_val,  y_pred_val)\n",
    "    fpr2, tpr2, thr2 = metrics.roc_curve(y_test,  y_pred_test)\n",
    "    auc2 = metrics.roc_auc_score(y_test,  y_pred_test)\n",
    "    gmeans2 = np.sqrt(tpr2 * (1-fpr2))\n",
    "    ix2 = np.argmax(gmeans2)\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"AUC validate: %.4f\" % auc1)\n",
    "    print(\"AUC test: %.4f\" % auc2)\n",
    "    plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.4f\" % auc1))\n",
    "    plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.4f\" % auc2))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(pos_probs, threshold):\n",
    "    return (pos_probs >= threshold).astype('int')\n",
    "    \n",
    "def thresholding(y_val, y_pred_val):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_val,  y_pred_val[:, 1])\n",
    "    # get the best threshold\n",
    "    J = tpr - fpr\n",
    "    ix = np.argmax(J)\n",
    "    best_thresh = thresholds[ix]\n",
    "    print('Best Threshold=%f' % (best_thresh))\n",
    "    print('FPR: %.4f\\nTPR: %.4f' %(fpr[ix], tpr[ix]))\n",
    "    y_pred_val_new = to_labels(y_pred_val[:, 1], best_thresh)\n",
    "    return y_pred_val_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(param,split):\n",
    "    \n",
    "    loaded_model = joblib.load('../logisticregression/model/param{}/lr_model_split{}.pkl'.format(param,split))\n",
    "\n",
    "    print(\"############## validate set ################\")\n",
    "    y_pred_val_raw = loaded_model.predict_proba(x_val)\n",
    "    y_pred_val = thresholding(y_val, y_pred_val_raw)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "    print('Confusion Matrix')\n",
    "    print('-'*16)\n",
    "    print(cm_val,'\\n')\n",
    "    print('-'*16)\n",
    "\n",
    "    # Calculate score\n",
    "    print(split)\n",
    "    cal_score_all(cm_val)\n",
    "    \n",
    "    # plot confusion matrix\n",
    "    plot_cm(cm_val, which=\"Validate set\")\n",
    "    \n",
    "    # ################# test set ##################\n",
    "    label = ['benign','malignant']\n",
    "    print(\"################# test set ##################\")\n",
    "    y_pred_test_raw = loaded_model.predict_proba(x_test)\n",
    "    y_pred_test = thresholding(y_test, y_pred_test_raw)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "    print('Confusion Matrix')\n",
    "    print('-'*16)\n",
    "    print(cm_test,'\\n')\n",
    "    print('-'*16)\n",
    "\n",
    "    # Calculate score\n",
    "    print(split)\n",
    "    cal_score_all(cm_test)\n",
    "\n",
    "    # plot confusion matrix\n",
    "    plot_cm(cm_test,which=\"Test set\")\n",
    "\n",
    "    ################## plot ROC curve ########################\n",
    "    fpr1, tpr1, thr1 = metrics.roc_curve(y_val,  y_pred_val_raw[:,1])\n",
    "    auc1 = metrics.roc_auc_score(y_val,  y_pred_val_raw[:,1])*100\n",
    "    fpr2, tpr2, thr2 = metrics.roc_curve(y_test,  y_pred_test_raw[:,1])\n",
    "    auc2 = metrics.roc_auc_score(y_test,  y_pred_test_raw[:,1])*100\n",
    "    gmeans2 = np.sqrt(tpr2 * (1-fpr2))\n",
    "    ix2 = np.argmax(gmeans2)\n",
    "    print(\"---------------------------------------\")\n",
    "    print(\"AUC validate: %.2f%%\" % auc1)\n",
    "    print(\"AUC test: %.2f%%\" % auc2)\n",
    "    plt.plot(fpr1,tpr1,label=\"ROC valid, auc=\"+str(\"%.2f%%\" % auc1))\n",
    "    plt.plot(fpr2,tpr2,label=\"ROC test, auc=\"+str(\"%.2f%%\" % auc2))\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    \n",
    "    return cm_val,cm_test,auc1,auc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mean_SD(name,split1, split2, split3, split4, split5):\n",
    "    sensitivity_values = []\n",
    "    sensitivity_values.append(split1)\n",
    "    sensitivity_values.append(split2)\n",
    "    sensitivity_values.append(split3)\n",
    "    sensitivity_values.append(split4)\n",
    "    sensitivity_values.append(split5)\n",
    "\n",
    "    # calculating the mean of sample set\n",
    "    mean_sensitivity = statistics.mean(sensitivity_values)\n",
    "    # Calculate standard deviation of sensitivity\n",
    "    sensitivity_sd = statistics.stdev(sensitivity_values)\n",
    "    \n",
    "    # Print mean sensitivity with standard deviation\n",
    "    print(\"%s: %.2f ± %.2f%%\" % (name,mean_sensitivity, sensitivity_sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(split1_cm, split2_cm, split3_cm, split4_cm, split5_cm):\n",
    "    cm_val = split1_cm + split2_cm + split3_cm + split4_cm + split5_cm\n",
    "    # Calculate score\n",
    "    print(\"split1\")\n",
    "    sen_val_1 = sensitivity(split1_cm)\n",
    "    spec_val_1 = specificity(split1_cm)\n",
    "    acc_val_1 = accuracy(split1_cm)\n",
    "    pre_val_1 = precision(split1_cm)\n",
    "    f1_score_val_1 = f1(split1_cm)\n",
    "    print(\"\\nsplit2\")\n",
    "    sen_val_2 = sensitivity(split2_cm)\n",
    "    spec_val_2 = specificity(split2_cm)\n",
    "    acc_val_2 = accuracy(split2_cm)\n",
    "    pre_val_2 = precision(split2_cm)\n",
    "    f1_score_val_2 = f1(split2_cm)\n",
    "    print(\"\\nsplit3\")\n",
    "    sen_val_3 = sensitivity(split3_cm)\n",
    "    spec_val_3 = specificity(split3_cm)\n",
    "    acc_val_3 = accuracy(split3_cm)\n",
    "    pre_val_3 = precision(split3_cm)\n",
    "    f1_score_val_3 = f1(split3_cm)\n",
    "    print(\"\\nsplit4\")\n",
    "    sen_val_4 = sensitivity(split4_cm)\n",
    "    spec_val_4 = specificity(split4_cm)\n",
    "    acc_val_4 = accuracy(split4_cm)\n",
    "    pre_val_4 = precision(split4_cm)\n",
    "    f1_score_val_4 = f1(split4_cm)\n",
    "    print(\"\\nsplit5\")\n",
    "    sen_val_5 = sensitivity(split5_cm)\n",
    "    spec_val_5 = specificity(split5_cm)\n",
    "    acc_val_5 = accuracy(split5_cm)\n",
    "    pre_val_5 = precision(split5_cm)\n",
    "    f1_score_val_5 = f1(split5_cm)\n",
    "    print(\"\\noverall\")\n",
    "    Mean_SD(\"sensitivity\",sen_val_1, sen_val_2, sen_val_3, sen_val_4, sen_val_5)\n",
    "    Mean_SD(\"specificity\",spec_val_1, spec_val_2, spec_val_3, spec_val_4, spec_val_5)\n",
    "    Mean_SD(\"accuracy\",acc_val_1, acc_val_2, acc_val_3, acc_val_4, acc_val_5)\n",
    "    Mean_SD(\"precision\",pre_val_1, pre_val_2, pre_val_3, pre_val_4, pre_val_5)\n",
    "    Mean_SD(\"f1_score\",f1_score_val_1, f1_score_val_2, f1_score_val_3, f1_score_val_4, f1_score_val_5)\n",
    "    # plot confusion matrix\n",
    "    class_names = ['benign','malignant']\n",
    "    # Normalize confusion matrix to percentage\n",
    "    cm_norm_val = cm_val.astype('float') / cm_val.sum(axis=1)[:, np.newaxis]\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm_norm_val, interpolation='nearest', cmap=\"rocket_r\")\n",
    "    ax.grid(False)\n",
    "    # Add labels\n",
    "    ax.set(xticks=np.arange(cm_norm_val.shape[1]),\n",
    "           yticks=np.arange(cm_norm_val.shape[0]),\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "    ax.set_title(\"Validate set\", fontsize=16)\n",
    "    ax.set_ylabel('Actual', fontsize=16)\n",
    "    ax.set_xlabel('Predicted', fontsize=16)\n",
    "    # Add percentage and count values inside plot\n",
    "    thresh = cm_norm_val.max() / 2.\n",
    "    for i in range(cm_norm_val.shape[0]):\n",
    "        for j in range(cm_norm_val.shape[1]):\n",
    "            ax.text(j, i, '''{}\\n({:.2f}%)'''.format(cm_val[i,j], cm_norm_val[i, j]*100),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm_norm_val[i, j] > thresh else \"black\")\n",
    "    # plt.savefig('rf_model/validate_cm_{}.png'.format(split))\n",
    "    # plt.savefig('rf_model/validate_cm_best_param2_{}.png'.format(split))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../logisticregression/model/param1/lr_model_split1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m param \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      2\u001b[0m split \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> 3\u001b[0m param1_split1_cm_val, param1_split1_cm_test, param1_split1_AUC_val, param1_split1_AUC_test \u001b[39m=\u001b[39m evaluate(param,split)\n",
      "Cell \u001b[1;32mIn[63], line 3\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(param, split)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(param,split):\n\u001b[1;32m----> 3\u001b[0m     loaded_model \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m../logisticregression/model/param\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m/lr_model_split\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(param,split))\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m############## validate set ################\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     y_pred_val_raw \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39mpredict_proba(x_val)\n",
      "File \u001b[1;32mc:\\Users\\net\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[39m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[39mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[39mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../logisticregression/model/param1/lr_model_split1.pkl'"
     ]
    }
   ],
   "source": [
    "param = 1\n",
    "split = 1\n",
    "param1_split1_cm_val, param1_split1_cm_test, param1_split1_AUC_val, param1_split1_AUC_test = evaluate(param,split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
